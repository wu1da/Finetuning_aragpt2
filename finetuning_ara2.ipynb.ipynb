{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10272303,"sourceType":"datasetVersion","datasetId":6355841}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n!pip install transformers datasets torch torchmetrics","metadata":{"id":"3d88SEZCi02n","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:21:56.682411Z","iopub.execute_input":"2024-12-22T20:21:56.682652Z","iopub.status.idle":"2024-12-22T20:22:01.307275Z","shell.execute_reply.started":"2024-12-22T20:21:56.682630Z","shell.execute_reply":"2024-12-22T20:22:01.306294Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport torchmetrics","metadata":{"id":"CKbijPOnScAx","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:22:01.308169Z","iopub.execute_input":"2024-12-22T20:22:01.308379Z","iopub.status.idle":"2024-12-22T20:22:16.239581Z","shell.execute_reply.started":"2024-12-22T20:22:01.308362Z","shell.execute_reply":"2024-12-22T20:22:16.238903Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import json\n\n# Load the dataset\ndef load_squad_format(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n       data = json.load(f)\n\n    contexts = []\n    questions = []\n    answers = []\n\n    for item in data[\"data\"]:\n        for paragraph in item[\"paragraphs\"]:\n            context = paragraph[\"context\"]\n            for qa in paragraph[\"qas\"]:\n                question = qa[\"question\"]\n                for answer in qa[\"answers\"]:\n                    contexts.append(context)\n                    questions.append(question)\n                    answers.append(answer[\"text\"])\n\n    return contexts, questions, answers\n\n# Load train and validation datasets\ntrain_contexts, train_questions, train_answers = load_squad_format('/kaggle/input/dataset-2/train.json')\nval_contexts, val_questions, val_answers = load_squad_format('/kaggle/input/dataset-2/val.json')\ntest_contexts, test_questions, test_answers = load_squad_format('/kaggle/input/dataset-2/test.json')","metadata":{"id":"IV66Jpo9TOQV","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:22:16.240438Z","iopub.execute_input":"2024-12-22T20:22:16.241036Z","iopub.status.idle":"2024-12-22T20:22:19.269908Z","shell.execute_reply.started":"2024-12-22T20:22:16.241011Z","shell.execute_reply":"2024-12-22T20:22:19.268988Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\ndf_train=pd.DataFrame()\ndf_train[\"context\"]=train_contexts\ndf_train[\"questions\"]=train_questions\ndf_train[\"answers\"]=train_answers\n\n\ndf_test=pd.DataFrame()\ndf_test[\"context\"]=test_contexts\ndf_test[\"questions\"]=test_questions\ndf_test[\"answers\"]=test_answers\n\ndf_val=pd.DataFrame()\ndf_val[\"context\"]=val_contexts\ndf_val[\"questions\"]=val_questions\ndf_val[\"answers\"]=val_answers\n\ndf_val.head(5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"fMneV2UaTnZu","outputId":"83bc7874-513f-432d-f9ff-1d966c5cf595","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:22:19.270812Z","iopub.execute_input":"2024-12-22T20:22:19.271124Z","iopub.status.idle":"2024-12-22T20:22:19.322523Z","shell.execute_reply.started":"2024-12-22T20:22:19.271093Z","shell.execute_reply":"2024-12-22T20:22:19.321818Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                             context  \\\n0  ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³Øª...   \n1  ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³Øª...   \n2  ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³Øª...   \n3  ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³Øª...   \n4  ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³Øª...   \n\n                                           questions  \\\n0                    Ù…Ø§ Ù‡ÙŠ ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ ÙˆØ£ÙŠÙ† ØªÙˆØ¬Ø¯ØŸ\\n   \n1  Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© Ø§Ù„Ø£Ø±Ø¨Ø¹Ø© Ø§Ù„ØªÙŠ ØªÙ…ÙŠØ² ØºØ§Ø¨Ø§Øª ...   \n2       ÙƒÙŠÙ ÙŠØªÙ… ØªØµÙ†ÙŠÙ ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ø¥Ù„Ù‰ Ø£ØºØ§Ø¨ ÙˆØ±Ø·Ø¨Ø©ØŸ\\n   \n3  ÙƒÙŠÙ ØªØ³Ø§Ø¹Ø¯ ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ ÙÙŠ ØªÙˆÙÙŠØ± Ø³ÙØ¨Ù„ Ø§Ù„Ø¹ÙŠØ´ Ù„Ù„Ø³...   \n4  Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ØºÙŠØ± Ø§Ù„Ø®Ø´Ø¨ÙŠØ© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„...   \n\n                                             answers  \n0  Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© Ø¨ÙŠÙˆÙ„ÙˆØ¬ÙŠ...  \n1   ÙŠÙÙ…ÙƒÙ† ØªØµÙ†ÙŠÙ ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ø¹Ù„Ù‰ Ø£Ù†Ù‡Ø§ Ø¬Ø§ÙØ©Ù‹ Ø£Ùˆ Ø±Ø·...  \n2  Ù‡ÙŠ ØªÙ„Ùƒ Ø§Ù„ØªÙŠ ØªØ³ØªÙ‚Ø¨Ù„ Ø£ÙƒØ«Ø± Ù…Ù† 1000 Ù…Ù… Ø³Ù†ÙˆÙŠÙ‹Ø§ Ù„Ù‡Ø·Ùˆ...  \n3       Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹ØªÙ…Ø¯ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ù† Ø§Ù„ØºØ§Ø¨Ø§Øª  \n4        Ù…Ø«Ù„ Ø§Ù„ÙØ§ÙƒÙ‡Ø© ÙˆØ§Ù„Ø¹Ø³Ù„ ÙˆØ¹Ù„Ù Ø§Ù„Ù…Ø§Ø´ÙŠØ© ÙˆØ­Ø·Ø¨ Ø§Ù„ÙˆÙ‚ÙˆØ¯  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>questions</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³Øª...</td>\n      <td>Ù…Ø§ Ù‡ÙŠ ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ ÙˆØ£ÙŠÙ† ØªÙˆØ¬Ø¯ØŸ\\n</td>\n      <td>Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© Ø¨ÙŠÙˆÙ„ÙˆØ¬ÙŠ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³Øª...</td>\n      <td>Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø¨ÙŠØ¦ÙŠØ© Ø§Ù„Ø£Ø±Ø¨Ø¹Ø© Ø§Ù„ØªÙŠ ØªÙ…ÙŠØ² ØºØ§Ø¨Ø§Øª ...</td>\n      <td>ÙŠÙÙ…ÙƒÙ† ØªØµÙ†ÙŠÙ ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ø¹Ù„Ù‰ Ø£Ù†Ù‡Ø§ Ø¬Ø§ÙØ©Ù‹ Ø£Ùˆ Ø±Ø·...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³Øª...</td>\n      <td>ÙƒÙŠÙ ÙŠØªÙ… ØªØµÙ†ÙŠÙ ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ø¥Ù„Ù‰ Ø£ØºØ§Ø¨ ÙˆØ±Ø·Ø¨Ø©ØŸ\\n</td>\n      <td>Ù‡ÙŠ ØªÙ„Ùƒ Ø§Ù„ØªÙŠ ØªØ³ØªÙ‚Ø¨Ù„ Ø£ÙƒØ«Ø± Ù…Ù† 1000 Ù…Ù… Ø³Ù†ÙˆÙŠÙ‹Ø§ Ù„Ù‡Ø·Ùˆ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³Øª...</td>\n      <td>ÙƒÙŠÙ ØªØ³Ø§Ø¹Ø¯ ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ ÙÙŠ ØªÙˆÙÙŠØ± Ø³ÙØ¨Ù„ Ø§Ù„Ø¹ÙŠØ´ Ù„Ù„Ø³...</td>\n      <td>Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹ØªÙ…Ø¯ÙˆÙ† Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ù† Ø§Ù„ØºØ§Ø¨Ø§Øª</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ØºØ§Ø¨Ø§Øª Ù…ÙŠÙˆÙ…Ø¨Ùˆ Ù‡ÙŠ Ø£Ø±Ø§Ø¶ÙŠ Ø¹ÙØ´Ø¨ÙŠØ© Ø§Ø³ØªÙˆØ§Ø¦ÙŠØ© ÙˆØ´Ø¨Ù‡ Ø§Ø³Øª...</td>\n      <td>Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ØºÙŠØ± Ø§Ù„Ø®Ø´Ø¨ÙŠØ© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„...</td>\n      <td>Ù…Ø«Ù„ Ø§Ù„ÙØ§ÙƒÙ‡Ø© ÙˆØ§Ù„Ø¹Ø³Ù„ ÙˆØ¹Ù„Ù Ø§Ù„Ù…Ø§Ø´ÙŠØ© ÙˆØ­Ø·Ø¨ Ø§Ù„ÙˆÙ‚ÙˆØ¯</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"print(f\"Contexts: {len(train_contexts)}\")\nprint(f\"Questions: {len(train_questions)}\")\nprint(f\"Answers: {len(train_answers)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8avWSLkvUE1L","outputId":"16262494-f8ba-4565-8d73-d35f97f9e134","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:22:19.323298Z","iopub.execute_input":"2024-12-22T20:22:19.323530Z","iopub.status.idle":"2024-12-22T20:22:19.328580Z","shell.execute_reply.started":"2024-12-22T20:22:19.323501Z","shell.execute_reply":"2024-12-22T20:22:19.327787Z"}},"outputs":[{"name":"stdout","text":"Contexts: 64782\nQuestions: 64782\nAnswers: 64782\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n\n# Load the pre-trained Arabic GPT-2 model and tokenizer\nmodel_name = \"aubmindlab/aragpt2-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DKcB0yTRSiJZ","outputId":"22a18bff-09a8-4785-9898-ff0243fe74fa","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:22:19.330879Z","iopub.execute_input":"2024-12-22T20:22:19.331167Z","iopub.status.idle":"2024-12-22T20:22:23.255140Z","shell.execute_reply.started":"2024-12-22T20:22:19.331147Z","shell.execute_reply":"2024-12-22T20:22:23.254459Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a52869d49afc4fb0888c34434a20edf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a530178945794d25a84bcdf40ce1f2dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbf6df4d7d984c648dfb63078a4cdd8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7a2bbb56f724ee899728bab2e114e6f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d61f11a650a14bf1a9c61d80d070937c"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"\n# Add a new [PAD] token\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\ndef tokenize_data(contexts, questions, answers, tokenizer, max_length=128):\n    input_texts = [\n        f\"Ø§Ù„Ø³Ø¤Ø§Ù„: {str(q).encode('utf-8').decode('utf-8')} \"\n        f\"Ø§Ù„Ø³ÙŠØ§Ù‚: {str(c).encode('utf-8').decode('utf-8')}\"\n        for q, c in zip(questions, contexts)\n    ]\n    \n    # Ensure answers are properly encoded\n    outputs = [str(a).encode('utf-8').decode('utf-8') for a in answers]\n\n    inputs = tokenizer(\n        input_texts,\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    outputs = tokenizer(\n        outputs,\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n\n    # Set labels to outputs for GPT-2 fine-tuning\n    inputs[\"labels\"] = outputs[\"input_ids\"]\n    return inputs\n\n\n# Tokenize train and validation data\ntrain_data = tokenize_data(train_contexts, train_questions, train_answers, tokenizer)\nval_data = tokenize_data(val_contexts, val_questions, val_answers, tokenizer)\ntest_data = tokenize_data(test_contexts, test_questions, test_answers, tokenizer)\n","metadata":{"id":"Y2fh2c3sT--6","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:22:23.256234Z","iopub.execute_input":"2024-12-22T20:22:23.256458Z","iopub.status.idle":"2024-12-22T20:24:14.041244Z","shell.execute_reply.started":"2024-12-22T20:22:23.256439Z","shell.execute_reply":"2024-12-22T20:24:14.040533Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(f\"Input IDs shape: {train_data['input_ids'].shape}\")\nprint(f\"Labels shape: {train_data['labels'].shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r3isch9qU4Go","outputId":"462be6f3-8eea-4eac-8721-3b2ae19945c8","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:24:14.042084Z","iopub.execute_input":"2024-12-22T20:24:14.042375Z","iopub.status.idle":"2024-12-22T20:24:14.046316Z","shell.execute_reply.started":"2024-12-22T20:24:14.042346Z","shell.execute_reply":"2024-12-22T20:24:14.045430Z"}},"outputs":[{"name":"stdout","text":"Input IDs shape: torch.Size([64782, 128])\nLabels shape: torch.Size([64782, 128])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nclass QADataset(Dataset):\n    def __init__(self, inputs):\n        self.input_ids = inputs[\"input_ids\"]\n        self.attention_mask = inputs[\"attention_mask\"]\n        self.labels = inputs[\"labels\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n            \"labels\": self.labels[idx],\n        }\n\n# Create datasets\ntrain_dataset = QADataset(train_data)\nval_dataset = QADataset(val_data)\ntest_dataset = QADataset(test_data)","metadata":{"id":"UdexahYyWcOf","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:24:14.047159Z","iopub.execute_input":"2024-12-22T20:24:14.047378Z","iopub.status.idle":"2024-12-22T20:24:14.065645Z","shell.execute_reply.started":"2024-12-22T20:24:14.047359Z","shell.execute_reply":"2024-12-22T20:24:14.064935Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model.resize_token_embeddings(len(tokenizer))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sr4pvKg6aj6C","outputId":"8b1b9414-3ebd-4338-8065-72f6731b2d97","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:24:14.066514Z","iopub.execute_input":"2024-12-22T20:24:14.066838Z","iopub.status.idle":"2024-12-22T20:24:14.850791Z","shell.execute_reply.started":"2024-12-22T20:24:14.066806Z","shell.execute_reply":"2024-12-22T20:24:14.850000Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Embedding(64001, 768)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,  # Ensure the model aligns with the tokenizer\n    padding=\"max_length\",  # Force padding to the max sequence length\n)","metadata":{"id":"gzbzxOUmapPi","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:24:14.851546Z","iopub.execute_input":"2024-12-22T20:24:14.851793Z","iopub.status.idle":"2024-12-22T20:24:14.855486Z","shell.execute_reply.started":"2024-12-22T20:24:14.851774Z","shell.execute_reply":"2024-12-22T20:24:14.854689Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./arabic-qa-gpt2-new\",\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n     # Add gradient accumulation to handle memory\n    gradient_accumulation_steps=4,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_steps=500,\n    eval_steps=500,\n    evaluation_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"loss\",\n    greater_is_better=False,\n    learning_rate=5e-5,\n    fp16=True,  # Use mixed precision training\n    dataloader_pin_memory=False,  # Prevent memory issues\n    remove_unused_columns=False,  # Keep all columns\n    report_to=\"none\"\n)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSPBQC6Jatbb","outputId":"2a069339-6124-4eba-808f-60242c2a002a","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T21:24:13.135251Z","iopub.execute_input":"2024-12-22T21:24:13.135546Z","iopub.status.idle":"2024-12-22T21:24:13.163762Z","shell.execute_reply.started":"2024-12-22T21:24:13.135525Z","shell.execute_reply":"2024-12-22T21:24:13.162959Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, TrainerCallback\nimport torch\n\nclass TrainingMonitorCallback(TrainerCallback):\n    def on_epoch_begin(self, args, state, control, **kwargs):\n        print(f\"\\n=== Starting Epoch: {state.epoch + 1}/{int(args.num_train_epochs)} ===\\n\")\n\n    def on_step_begin(self, args, state, control, **kwargs):\n        print(f\"--- Step: {state.global_step}/{state.max_steps} ---\")\n\n    def on_train_batch_begin(self, args, state, control, train_dataloader, **kwargs):\n        print(f\"Processing Batch {state.global_step}...\")\n\n# Initialize trainer\ndef initialize_trainer(model, train_dataset, val_dataset):\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        callbacks=[TrainingMonitorCallback()],\n        data_collator=data_collator\n    )\n    return trainer","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"uKYv0crTa0va","outputId":"7ab8cf10-d464-4cdb-e840-038e5be34026","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:24:14.976588Z","iopub.execute_input":"2024-12-22T20:24:14.976821Z","iopub.status.idle":"2024-12-22T20:24:14.982415Z","shell.execute_reply.started":"2024-12-22T20:24:14.976798Z","shell.execute_reply":"2024-12-22T20:24:14.981639Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_model(train_dataset, val_dataset, test_dataset):\n\n\n    # Initialize and start training\n    trainer = initialize_trainer(model, train_dataset, val_dataset)\n    trainer.train()\n\n    # Evaluate on test set\n    test_results = trainer.evaluate(test_dataset)\n    print(f\"Test Results: {test_results}\")\n\n    # Save the fine-tuned model\n    model.save_pretrained(\"./arabic-qa-gpt2-finetuned\")\n    tokenizer.save_pretrained(\"./arabic-qa-gpt2-finetuned\")\n\n    return test_results\n\n","metadata":{"id":"CUjnRxQVbDGt","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:24:14.983205Z","iopub.execute_input":"2024-12-22T20:24:14.983490Z","iopub.status.idle":"2024-12-22T20:24:14.999562Z","shell.execute_reply.started":"2024-12-22T20:24:14.983460Z","shell.execute_reply":"2024-12-22T20:24:14.998755Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"results = train_model(train_dataset, val_dataset, test_dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"49AscaNjbhD1","outputId":"7c743aad-edfa-4608-98d0-df7b3b047136","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T20:24:15.000249Z","iopub.execute_input":"2024-12-22T20:24:15.000447Z","iopub.status.idle":"2024-12-22T20:55:57.389830Z","shell.execute_reply.started":"2024-12-22T20:24:15.000429Z","shell.execute_reply":"2024-12-22T20:55:57.388790Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nAsking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n","output_type":"stream"},{"name":"stdout","text":"\n=== Starting Epoch: 1/1 ===\n\n--- Step: 0/2024 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2024' max='2024' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2024/2024 29:59, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.016900</td>\n      <td>0.971867</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.916400</td>\n      <td>0.933964</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.910000</td>\n      <td>0.917943</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.880600</td>\n      <td>0.915721</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"--- Step: 1/2024 ---\n--- Step: 2/2024 ---\n--- Step: 3/2024 ---\n--- Step: 4/2024 ---\n--- Step: 5/2024 ---\n--- Step: 6/2024 ---\n--- Step: 7/2024 ---\n--- Step: 8/2024 ---\n--- Step: 9/2024 ---\n--- Step: 10/2024 ---\n--- Step: 11/2024 ---\n--- Step: 12/2024 ---\n--- Step: 13/2024 ---\n--- Step: 14/2024 ---\n--- Step: 15/2024 ---\n--- Step: 16/2024 ---\n--- Step: 17/2024 ---\n--- Step: 18/2024 ---\n--- Step: 19/2024 ---\n--- Step: 20/2024 ---\n--- Step: 21/2024 ---\n--- Step: 22/2024 ---\n--- Step: 23/2024 ---\n--- Step: 24/2024 ---\n--- Step: 25/2024 ---\n--- Step: 26/2024 ---\n--- Step: 27/2024 ---\n--- Step: 28/2024 ---\n--- Step: 29/2024 ---\n--- Step: 30/2024 ---\n--- Step: 31/2024 ---\n--- Step: 32/2024 ---\n--- Step: 33/2024 ---\n--- Step: 34/2024 ---\n--- Step: 35/2024 ---\n--- Step: 36/2024 ---\n--- Step: 37/2024 ---\n--- Step: 38/2024 ---\n--- Step: 39/2024 ---\n--- Step: 40/2024 ---\n--- Step: 41/2024 ---\n--- Step: 42/2024 ---\n--- Step: 43/2024 ---\n--- Step: 44/2024 ---\n--- Step: 45/2024 ---\n--- Step: 46/2024 ---\n--- Step: 47/2024 ---\n--- Step: 48/2024 ---\n--- Step: 49/2024 ---\n--- Step: 50/2024 ---\n--- Step: 51/2024 ---\n--- Step: 52/2024 ---\n--- Step: 53/2024 ---\n--- Step: 54/2024 ---\n--- Step: 55/2024 ---\n--- Step: 56/2024 ---\n--- Step: 57/2024 ---\n--- Step: 58/2024 ---\n--- Step: 59/2024 ---\n--- Step: 60/2024 ---\n--- Step: 61/2024 ---\n--- Step: 62/2024 ---\n--- Step: 63/2024 ---\n--- Step: 64/2024 ---\n--- Step: 65/2024 ---\n--- Step: 66/2024 ---\n--- Step: 67/2024 ---\n--- Step: 68/2024 ---\n--- Step: 69/2024 ---\n--- Step: 70/2024 ---\n--- Step: 71/2024 ---\n--- Step: 72/2024 ---\n--- Step: 73/2024 ---\n--- Step: 74/2024 ---\n--- Step: 75/2024 ---\n--- Step: 76/2024 ---\n--- Step: 77/2024 ---\n--- Step: 78/2024 ---\n--- Step: 79/2024 ---\n--- Step: 80/2024 ---\n--- Step: 81/2024 ---\n--- Step: 82/2024 ---\n--- Step: 83/2024 ---\n--- Step: 84/2024 ---\n--- Step: 85/2024 ---\n--- Step: 86/2024 ---\n--- Step: 87/2024 ---\n--- Step: 88/2024 ---\n--- Step: 89/2024 ---\n--- Step: 90/2024 ---\n--- Step: 91/2024 ---\n--- Step: 92/2024 ---\n--- Step: 93/2024 ---\n--- Step: 94/2024 ---\n--- Step: 95/2024 ---\n--- Step: 96/2024 ---\n--- Step: 97/2024 ---\n--- Step: 98/2024 ---\n--- Step: 99/2024 ---\n--- Step: 100/2024 ---\n--- Step: 101/2024 ---\n--- Step: 102/2024 ---\n--- Step: 103/2024 ---\n--- Step: 104/2024 ---\n--- Step: 105/2024 ---\n--- Step: 106/2024 ---\n--- Step: 107/2024 ---\n--- Step: 108/2024 ---\n--- Step: 109/2024 ---\n--- Step: 110/2024 ---\n--- Step: 111/2024 ---\n--- Step: 112/2024 ---\n--- Step: 113/2024 ---\n--- Step: 114/2024 ---\n--- Step: 115/2024 ---\n--- Step: 116/2024 ---\n--- Step: 117/2024 ---\n--- Step: 118/2024 ---\n--- Step: 119/2024 ---\n--- Step: 120/2024 ---\n--- Step: 121/2024 ---\n--- Step: 122/2024 ---\n--- Step: 123/2024 ---\n--- Step: 124/2024 ---\n--- Step: 125/2024 ---\n--- Step: 126/2024 ---\n--- Step: 127/2024 ---\n--- Step: 128/2024 ---\n--- Step: 129/2024 ---\n--- Step: 130/2024 ---\n--- Step: 131/2024 ---\n--- Step: 132/2024 ---\n--- Step: 133/2024 ---\n--- Step: 134/2024 ---\n--- Step: 135/2024 ---\n--- Step: 136/2024 ---\n--- Step: 137/2024 ---\n--- Step: 138/2024 ---\n--- Step: 139/2024 ---\n--- Step: 140/2024 ---\n--- Step: 141/2024 ---\n--- Step: 142/2024 ---\n--- Step: 143/2024 ---\n--- Step: 144/2024 ---\n--- Step: 145/2024 ---\n--- Step: 146/2024 ---\n--- Step: 147/2024 ---\n--- Step: 148/2024 ---\n--- Step: 149/2024 ---\n--- Step: 150/2024 ---\n--- Step: 151/2024 ---\n--- Step: 152/2024 ---\n--- Step: 153/2024 ---\n--- Step: 154/2024 ---\n--- Step: 155/2024 ---\n--- Step: 156/2024 ---\n--- Step: 157/2024 ---\n--- Step: 158/2024 ---\n--- Step: 159/2024 ---\n--- Step: 160/2024 ---\n--- Step: 161/2024 ---\n--- Step: 162/2024 ---\n--- Step: 163/2024 ---\n--- Step: 164/2024 ---\n--- Step: 165/2024 ---\n--- Step: 166/2024 ---\n--- Step: 167/2024 ---\n--- Step: 168/2024 ---\n--- Step: 169/2024 ---\n--- Step: 170/2024 ---\n--- Step: 171/2024 ---\n--- Step: 172/2024 ---\n--- Step: 173/2024 ---\n--- Step: 174/2024 ---\n--- Step: 175/2024 ---\n--- Step: 176/2024 ---\n--- Step: 177/2024 ---\n--- Step: 178/2024 ---\n--- Step: 179/2024 ---\n--- Step: 180/2024 ---\n--- Step: 181/2024 ---\n--- Step: 182/2024 ---\n--- Step: 183/2024 ---\n--- Step: 184/2024 ---\n--- Step: 185/2024 ---\n--- Step: 186/2024 ---\n--- Step: 187/2024 ---\n--- Step: 188/2024 ---\n--- Step: 189/2024 ---\n--- Step: 190/2024 ---\n--- Step: 191/2024 ---\n--- Step: 192/2024 ---\n--- Step: 193/2024 ---\n--- Step: 194/2024 ---\n--- Step: 195/2024 ---\n--- Step: 196/2024 ---\n--- Step: 197/2024 ---\n--- Step: 198/2024 ---\n--- Step: 199/2024 ---\n--- Step: 200/2024 ---\n--- Step: 201/2024 ---\n--- Step: 202/2024 ---\n--- Step: 203/2024 ---\n--- Step: 204/2024 ---\n--- Step: 205/2024 ---\n--- Step: 206/2024 ---\n--- Step: 207/2024 ---\n--- Step: 208/2024 ---\n--- Step: 209/2024 ---\n--- Step: 210/2024 ---\n--- Step: 211/2024 ---\n--- Step: 212/2024 ---\n--- Step: 213/2024 ---\n--- Step: 214/2024 ---\n--- Step: 215/2024 ---\n--- Step: 216/2024 ---\n--- Step: 217/2024 ---\n--- Step: 218/2024 ---\n--- Step: 219/2024 ---\n--- Step: 220/2024 ---\n--- Step: 221/2024 ---\n--- Step: 222/2024 ---\n--- Step: 223/2024 ---\n--- Step: 224/2024 ---\n--- Step: 225/2024 ---\n--- Step: 226/2024 ---\n--- Step: 227/2024 ---\n--- Step: 228/2024 ---\n--- Step: 229/2024 ---\n--- Step: 230/2024 ---\n--- Step: 231/2024 ---\n--- Step: 232/2024 ---\n--- Step: 233/2024 ---\n--- Step: 234/2024 ---\n--- Step: 235/2024 ---\n--- Step: 236/2024 ---\n--- Step: 237/2024 ---\n--- Step: 238/2024 ---\n--- Step: 239/2024 ---\n--- Step: 240/2024 ---\n--- Step: 241/2024 ---\n--- Step: 242/2024 ---\n--- Step: 243/2024 ---\n--- Step: 244/2024 ---\n--- Step: 245/2024 ---\n--- Step: 246/2024 ---\n--- Step: 247/2024 ---\n--- Step: 248/2024 ---\n--- Step: 249/2024 ---\n--- Step: 250/2024 ---\n--- Step: 251/2024 ---\n--- Step: 252/2024 ---\n--- Step: 253/2024 ---\n--- Step: 254/2024 ---\n--- Step: 255/2024 ---\n--- Step: 256/2024 ---\n--- Step: 257/2024 ---\n--- Step: 258/2024 ---\n--- Step: 259/2024 ---\n--- Step: 260/2024 ---\n--- Step: 261/2024 ---\n--- Step: 262/2024 ---\n--- Step: 263/2024 ---\n--- Step: 264/2024 ---\n--- Step: 265/2024 ---\n--- Step: 266/2024 ---\n--- Step: 267/2024 ---\n--- Step: 268/2024 ---\n--- Step: 269/2024 ---\n--- Step: 270/2024 ---\n--- Step: 271/2024 ---\n--- Step: 272/2024 ---\n--- Step: 273/2024 ---\n--- Step: 274/2024 ---\n--- Step: 275/2024 ---\n--- Step: 276/2024 ---\n--- Step: 277/2024 ---\n--- Step: 278/2024 ---\n--- Step: 279/2024 ---\n--- Step: 280/2024 ---\n--- Step: 281/2024 ---\n--- Step: 282/2024 ---\n--- Step: 283/2024 ---\n--- Step: 284/2024 ---\n--- Step: 285/2024 ---\n--- Step: 286/2024 ---\n--- Step: 287/2024 ---\n--- Step: 288/2024 ---\n--- Step: 289/2024 ---\n--- Step: 290/2024 ---\n--- Step: 291/2024 ---\n--- Step: 292/2024 ---\n--- Step: 293/2024 ---\n--- Step: 294/2024 ---\n--- Step: 295/2024 ---\n--- Step: 296/2024 ---\n--- Step: 297/2024 ---\n--- Step: 298/2024 ---\n--- Step: 299/2024 ---\n--- Step: 300/2024 ---\n--- Step: 301/2024 ---\n--- Step: 302/2024 ---\n--- Step: 303/2024 ---\n--- Step: 304/2024 ---\n--- Step: 305/2024 ---\n--- Step: 306/2024 ---\n--- Step: 307/2024 ---\n--- Step: 308/2024 ---\n--- Step: 309/2024 ---\n--- Step: 310/2024 ---\n--- Step: 311/2024 ---\n--- Step: 312/2024 ---\n--- Step: 313/2024 ---\n--- Step: 314/2024 ---\n--- Step: 315/2024 ---\n--- Step: 316/2024 ---\n--- Step: 317/2024 ---\n--- Step: 318/2024 ---\n--- Step: 319/2024 ---\n--- Step: 320/2024 ---\n--- Step: 321/2024 ---\n--- Step: 322/2024 ---\n--- Step: 323/2024 ---\n--- Step: 324/2024 ---\n--- Step: 325/2024 ---\n--- Step: 326/2024 ---\n--- Step: 327/2024 ---\n--- Step: 328/2024 ---\n--- Step: 329/2024 ---\n--- Step: 330/2024 ---\n--- Step: 331/2024 ---\n--- Step: 332/2024 ---\n--- Step: 333/2024 ---\n--- Step: 334/2024 ---\n--- Step: 335/2024 ---\n--- Step: 336/2024 ---\n--- Step: 337/2024 ---\n--- Step: 338/2024 ---\n--- Step: 339/2024 ---\n--- Step: 340/2024 ---\n--- Step: 341/2024 ---\n--- Step: 342/2024 ---\n--- Step: 343/2024 ---\n--- Step: 344/2024 ---\n--- Step: 345/2024 ---\n--- Step: 346/2024 ---\n--- Step: 347/2024 ---\n--- Step: 348/2024 ---\n--- Step: 349/2024 ---\n--- Step: 350/2024 ---\n--- Step: 351/2024 ---\n--- Step: 352/2024 ---\n--- Step: 353/2024 ---\n--- Step: 354/2024 ---\n--- Step: 355/2024 ---\n--- Step: 356/2024 ---\n--- Step: 357/2024 ---\n--- Step: 358/2024 ---\n--- Step: 359/2024 ---\n--- Step: 360/2024 ---\n--- Step: 361/2024 ---\n--- Step: 362/2024 ---\n--- Step: 363/2024 ---\n--- Step: 364/2024 ---\n--- Step: 365/2024 ---\n--- Step: 366/2024 ---\n--- Step: 367/2024 ---\n--- Step: 368/2024 ---\n--- Step: 369/2024 ---\n--- Step: 370/2024 ---\n--- Step: 371/2024 ---\n--- Step: 372/2024 ---\n--- Step: 373/2024 ---\n--- Step: 374/2024 ---\n--- Step: 375/2024 ---\n--- Step: 376/2024 ---\n--- Step: 377/2024 ---\n--- Step: 378/2024 ---\n--- Step: 379/2024 ---\n--- Step: 380/2024 ---\n--- Step: 381/2024 ---\n--- Step: 382/2024 ---\n--- Step: 383/2024 ---\n--- Step: 384/2024 ---\n--- Step: 385/2024 ---\n--- Step: 386/2024 ---\n--- Step: 387/2024 ---\n--- Step: 388/2024 ---\n--- Step: 389/2024 ---\n--- Step: 390/2024 ---\n--- Step: 391/2024 ---\n--- Step: 392/2024 ---\n--- Step: 393/2024 ---\n--- Step: 394/2024 ---\n--- Step: 395/2024 ---\n--- Step: 396/2024 ---\n--- Step: 397/2024 ---\n--- Step: 398/2024 ---\n--- Step: 399/2024 ---\n--- Step: 400/2024 ---\n--- Step: 401/2024 ---\n--- Step: 402/2024 ---\n--- Step: 403/2024 ---\n--- Step: 404/2024 ---\n--- Step: 405/2024 ---\n--- Step: 406/2024 ---\n--- Step: 407/2024 ---\n--- Step: 408/2024 ---\n--- Step: 409/2024 ---\n--- Step: 410/2024 ---\n--- Step: 411/2024 ---\n--- Step: 412/2024 ---\n--- Step: 413/2024 ---\n--- Step: 414/2024 ---\n--- Step: 415/2024 ---\n--- Step: 416/2024 ---\n--- Step: 417/2024 ---\n--- Step: 418/2024 ---\n--- Step: 419/2024 ---\n--- Step: 420/2024 ---\n--- Step: 421/2024 ---\n--- Step: 422/2024 ---\n--- Step: 423/2024 ---\n--- Step: 424/2024 ---\n--- Step: 425/2024 ---\n--- Step: 426/2024 ---\n--- Step: 427/2024 ---\n--- Step: 428/2024 ---\n--- Step: 429/2024 ---\n--- Step: 430/2024 ---\n--- Step: 431/2024 ---\n--- Step: 432/2024 ---\n--- Step: 433/2024 ---\n--- Step: 434/2024 ---\n--- Step: 435/2024 ---\n--- Step: 436/2024 ---\n--- Step: 437/2024 ---\n--- Step: 438/2024 ---\n--- Step: 439/2024 ---\n--- Step: 440/2024 ---\n--- Step: 441/2024 ---\n--- Step: 442/2024 ---\n--- Step: 443/2024 ---\n--- Step: 444/2024 ---\n--- Step: 445/2024 ---\n--- Step: 446/2024 ---\n--- Step: 447/2024 ---\n--- Step: 448/2024 ---\n--- Step: 449/2024 ---\n--- Step: 450/2024 ---\n--- Step: 451/2024 ---\n--- Step: 452/2024 ---\n--- Step: 453/2024 ---\n--- Step: 454/2024 ---\n--- Step: 455/2024 ---\n--- Step: 456/2024 ---\n--- Step: 457/2024 ---\n--- Step: 458/2024 ---\n--- Step: 459/2024 ---\n--- Step: 460/2024 ---\n--- Step: 461/2024 ---\n--- Step: 462/2024 ---\n--- Step: 463/2024 ---\n--- Step: 464/2024 ---\n--- Step: 465/2024 ---\n--- Step: 466/2024 ---\n--- Step: 467/2024 ---\n--- Step: 468/2024 ---\n--- Step: 469/2024 ---\n--- Step: 470/2024 ---\n--- Step: 471/2024 ---\n--- Step: 472/2024 ---\n--- Step: 473/2024 ---\n--- Step: 474/2024 ---\n--- Step: 475/2024 ---\n--- Step: 476/2024 ---\n--- Step: 477/2024 ---\n--- Step: 478/2024 ---\n--- Step: 479/2024 ---\n--- Step: 480/2024 ---\n--- Step: 481/2024 ---\n--- Step: 482/2024 ---\n--- Step: 483/2024 ---\n--- Step: 484/2024 ---\n--- Step: 485/2024 ---\n--- Step: 486/2024 ---\n--- Step: 487/2024 ---\n--- Step: 488/2024 ---\n--- Step: 489/2024 ---\n--- Step: 490/2024 ---\n--- Step: 491/2024 ---\n--- Step: 492/2024 ---\n--- Step: 493/2024 ---\n--- Step: 494/2024 ---\n--- Step: 495/2024 ---\n--- Step: 496/2024 ---\n--- Step: 497/2024 ---\n--- Step: 498/2024 ---\n--- Step: 499/2024 ---\n--- Step: 500/2024 ---\n--- Step: 501/2024 ---\n--- Step: 502/2024 ---\n--- Step: 503/2024 ---\n--- Step: 504/2024 ---\n--- Step: 505/2024 ---\n--- Step: 506/2024 ---\n--- Step: 507/2024 ---\n--- Step: 508/2024 ---\n--- Step: 509/2024 ---\n--- Step: 510/2024 ---\n--- Step: 511/2024 ---\n--- Step: 512/2024 ---\n--- Step: 513/2024 ---\n--- Step: 514/2024 ---\n--- Step: 515/2024 ---\n--- Step: 516/2024 ---\n--- Step: 517/2024 ---\n--- Step: 518/2024 ---\n--- Step: 519/2024 ---\n--- Step: 520/2024 ---\n--- Step: 521/2024 ---\n--- Step: 522/2024 ---\n--- Step: 523/2024 ---\n--- Step: 524/2024 ---\n--- Step: 525/2024 ---\n--- Step: 526/2024 ---\n--- Step: 527/2024 ---\n--- Step: 528/2024 ---\n--- Step: 529/2024 ---\n--- Step: 530/2024 ---\n--- Step: 531/2024 ---\n--- Step: 532/2024 ---\n--- Step: 533/2024 ---\n--- Step: 534/2024 ---\n--- Step: 535/2024 ---\n--- Step: 536/2024 ---\n--- Step: 537/2024 ---\n--- Step: 538/2024 ---\n--- Step: 539/2024 ---\n--- Step: 540/2024 ---\n--- Step: 541/2024 ---\n--- Step: 542/2024 ---\n--- Step: 543/2024 ---\n--- Step: 544/2024 ---\n--- Step: 545/2024 ---\n--- Step: 546/2024 ---\n--- Step: 547/2024 ---\n--- Step: 548/2024 ---\n--- Step: 549/2024 ---\n--- Step: 550/2024 ---\n--- Step: 551/2024 ---\n--- Step: 552/2024 ---\n--- Step: 553/2024 ---\n--- Step: 554/2024 ---\n--- Step: 555/2024 ---\n--- Step: 556/2024 ---\n--- Step: 557/2024 ---\n--- Step: 558/2024 ---\n--- Step: 559/2024 ---\n--- Step: 560/2024 ---\n--- Step: 561/2024 ---\n--- Step: 562/2024 ---\n--- Step: 563/2024 ---\n--- Step: 564/2024 ---\n--- Step: 565/2024 ---\n--- Step: 566/2024 ---\n--- Step: 567/2024 ---\n--- Step: 568/2024 ---\n--- Step: 569/2024 ---\n--- Step: 570/2024 ---\n--- Step: 571/2024 ---\n--- Step: 572/2024 ---\n--- Step: 573/2024 ---\n--- Step: 574/2024 ---\n--- Step: 575/2024 ---\n--- Step: 576/2024 ---\n--- Step: 577/2024 ---\n--- Step: 578/2024 ---\n--- Step: 579/2024 ---\n--- Step: 580/2024 ---\n--- Step: 581/2024 ---\n--- Step: 582/2024 ---\n--- Step: 583/2024 ---\n--- Step: 584/2024 ---\n--- Step: 585/2024 ---\n--- Step: 586/2024 ---\n--- Step: 587/2024 ---\n--- Step: 588/2024 ---\n--- Step: 589/2024 ---\n--- Step: 590/2024 ---\n--- Step: 591/2024 ---\n--- Step: 592/2024 ---\n--- Step: 593/2024 ---\n--- Step: 594/2024 ---\n--- Step: 595/2024 ---\n--- Step: 596/2024 ---\n--- Step: 597/2024 ---\n--- Step: 598/2024 ---\n--- Step: 599/2024 ---\n--- Step: 600/2024 ---\n--- Step: 601/2024 ---\n--- Step: 602/2024 ---\n--- Step: 603/2024 ---\n--- Step: 604/2024 ---\n--- Step: 605/2024 ---\n--- Step: 606/2024 ---\n--- Step: 607/2024 ---\n--- Step: 608/2024 ---\n--- Step: 609/2024 ---\n--- Step: 610/2024 ---\n--- Step: 611/2024 ---\n--- Step: 612/2024 ---\n--- Step: 613/2024 ---\n--- Step: 614/2024 ---\n--- Step: 615/2024 ---\n--- Step: 616/2024 ---\n--- Step: 617/2024 ---\n--- Step: 618/2024 ---\n--- Step: 619/2024 ---\n--- Step: 620/2024 ---\n--- Step: 621/2024 ---\n--- Step: 622/2024 ---\n--- Step: 623/2024 ---\n--- Step: 624/2024 ---\n--- Step: 625/2024 ---\n--- Step: 626/2024 ---\n--- Step: 627/2024 ---\n--- Step: 628/2024 ---\n--- Step: 629/2024 ---\n--- Step: 630/2024 ---\n--- Step: 631/2024 ---\n--- Step: 632/2024 ---\n--- Step: 633/2024 ---\n--- Step: 634/2024 ---\n--- Step: 635/2024 ---\n--- Step: 636/2024 ---\n--- Step: 637/2024 ---\n--- Step: 638/2024 ---\n--- Step: 639/2024 ---\n--- Step: 640/2024 ---\n--- Step: 641/2024 ---\n--- Step: 642/2024 ---\n--- Step: 643/2024 ---\n--- Step: 644/2024 ---\n--- Step: 645/2024 ---\n--- Step: 646/2024 ---\n--- Step: 647/2024 ---\n--- Step: 648/2024 ---\n--- Step: 649/2024 ---\n--- Step: 650/2024 ---\n--- Step: 651/2024 ---\n--- Step: 652/2024 ---\n--- Step: 653/2024 ---\n--- Step: 654/2024 ---\n--- Step: 655/2024 ---\n--- Step: 656/2024 ---\n--- Step: 657/2024 ---\n--- Step: 658/2024 ---\n--- Step: 659/2024 ---\n--- Step: 660/2024 ---\n--- Step: 661/2024 ---\n--- Step: 662/2024 ---\n--- Step: 663/2024 ---\n--- Step: 664/2024 ---\n--- Step: 665/2024 ---\n--- Step: 666/2024 ---\n--- Step: 667/2024 ---\n--- Step: 668/2024 ---\n--- Step: 669/2024 ---\n--- Step: 670/2024 ---\n--- Step: 671/2024 ---\n--- Step: 672/2024 ---\n--- Step: 673/2024 ---\n--- Step: 674/2024 ---\n--- Step: 675/2024 ---\n--- Step: 676/2024 ---\n--- Step: 677/2024 ---\n--- Step: 678/2024 ---\n--- Step: 679/2024 ---\n--- Step: 680/2024 ---\n--- Step: 681/2024 ---\n--- Step: 682/2024 ---\n--- Step: 683/2024 ---\n--- Step: 684/2024 ---\n--- Step: 685/2024 ---\n--- Step: 686/2024 ---\n--- Step: 687/2024 ---\n--- Step: 688/2024 ---\n--- Step: 689/2024 ---\n--- Step: 690/2024 ---\n--- Step: 691/2024 ---\n--- Step: 692/2024 ---\n--- Step: 693/2024 ---\n--- Step: 694/2024 ---\n--- Step: 695/2024 ---\n--- Step: 696/2024 ---\n--- Step: 697/2024 ---\n--- Step: 698/2024 ---\n--- Step: 699/2024 ---\n--- Step: 700/2024 ---\n--- Step: 701/2024 ---\n--- Step: 702/2024 ---\n--- Step: 703/2024 ---\n--- Step: 704/2024 ---\n--- Step: 705/2024 ---\n--- Step: 706/2024 ---\n--- Step: 707/2024 ---\n--- Step: 708/2024 ---\n--- Step: 709/2024 ---\n--- Step: 710/2024 ---\n--- Step: 711/2024 ---\n--- Step: 712/2024 ---\n--- Step: 713/2024 ---\n--- Step: 714/2024 ---\n--- Step: 715/2024 ---\n--- Step: 716/2024 ---\n--- Step: 717/2024 ---\n--- Step: 718/2024 ---\n--- Step: 719/2024 ---\n--- Step: 720/2024 ---\n--- Step: 721/2024 ---\n--- Step: 722/2024 ---\n--- Step: 723/2024 ---\n--- Step: 724/2024 ---\n--- Step: 725/2024 ---\n--- Step: 726/2024 ---\n--- Step: 727/2024 ---\n--- Step: 728/2024 ---\n--- Step: 729/2024 ---\n--- Step: 730/2024 ---\n--- Step: 731/2024 ---\n--- Step: 732/2024 ---\n--- Step: 733/2024 ---\n--- Step: 734/2024 ---\n--- Step: 735/2024 ---\n--- Step: 736/2024 ---\n--- Step: 737/2024 ---\n--- Step: 738/2024 ---\n--- Step: 739/2024 ---\n--- Step: 740/2024 ---\n--- Step: 741/2024 ---\n--- Step: 742/2024 ---\n--- Step: 743/2024 ---\n--- Step: 744/2024 ---\n--- Step: 745/2024 ---\n--- Step: 746/2024 ---\n--- Step: 747/2024 ---\n--- Step: 748/2024 ---\n--- Step: 749/2024 ---\n--- Step: 750/2024 ---\n--- Step: 751/2024 ---\n--- Step: 752/2024 ---\n--- Step: 753/2024 ---\n--- Step: 754/2024 ---\n--- Step: 755/2024 ---\n--- Step: 756/2024 ---\n--- Step: 757/2024 ---\n--- Step: 758/2024 ---\n--- Step: 759/2024 ---\n--- Step: 760/2024 ---\n--- Step: 761/2024 ---\n--- Step: 762/2024 ---\n--- Step: 763/2024 ---\n--- Step: 764/2024 ---\n--- Step: 765/2024 ---\n--- Step: 766/2024 ---\n--- Step: 767/2024 ---\n--- Step: 768/2024 ---\n--- Step: 769/2024 ---\n--- Step: 770/2024 ---\n--- Step: 771/2024 ---\n--- Step: 772/2024 ---\n--- Step: 773/2024 ---\n--- Step: 774/2024 ---\n--- Step: 775/2024 ---\n--- Step: 776/2024 ---\n--- Step: 777/2024 ---\n--- Step: 778/2024 ---\n--- Step: 779/2024 ---\n--- Step: 780/2024 ---\n--- Step: 781/2024 ---\n--- Step: 782/2024 ---\n--- Step: 783/2024 ---\n--- Step: 784/2024 ---\n--- Step: 785/2024 ---\n--- Step: 786/2024 ---\n--- Step: 787/2024 ---\n--- Step: 788/2024 ---\n--- Step: 789/2024 ---\n--- Step: 790/2024 ---\n--- Step: 791/2024 ---\n--- Step: 792/2024 ---\n--- Step: 793/2024 ---\n--- Step: 794/2024 ---\n--- Step: 795/2024 ---\n--- Step: 796/2024 ---\n--- Step: 797/2024 ---\n--- Step: 798/2024 ---\n--- Step: 799/2024 ---\n--- Step: 800/2024 ---\n--- Step: 801/2024 ---\n--- Step: 802/2024 ---\n--- Step: 803/2024 ---\n--- Step: 804/2024 ---\n--- Step: 805/2024 ---\n--- Step: 806/2024 ---\n--- Step: 807/2024 ---\n--- Step: 808/2024 ---\n--- Step: 809/2024 ---\n--- Step: 810/2024 ---\n--- Step: 811/2024 ---\n--- Step: 812/2024 ---\n--- Step: 813/2024 ---\n--- Step: 814/2024 ---\n--- Step: 815/2024 ---\n--- Step: 816/2024 ---\n--- Step: 817/2024 ---\n--- Step: 818/2024 ---\n--- Step: 819/2024 ---\n--- Step: 820/2024 ---\n--- Step: 821/2024 ---\n--- Step: 822/2024 ---\n--- Step: 823/2024 ---\n--- Step: 824/2024 ---\n--- Step: 825/2024 ---\n--- Step: 826/2024 ---\n--- Step: 827/2024 ---\n--- Step: 828/2024 ---\n--- Step: 829/2024 ---\n--- Step: 830/2024 ---\n--- Step: 831/2024 ---\n--- Step: 832/2024 ---\n--- Step: 833/2024 ---\n--- Step: 834/2024 ---\n--- Step: 835/2024 ---\n--- Step: 836/2024 ---\n--- Step: 837/2024 ---\n--- Step: 838/2024 ---\n--- Step: 839/2024 ---\n--- Step: 840/2024 ---\n--- Step: 841/2024 ---\n--- Step: 842/2024 ---\n--- Step: 843/2024 ---\n--- Step: 844/2024 ---\n--- Step: 845/2024 ---\n--- Step: 846/2024 ---\n--- Step: 847/2024 ---\n--- Step: 848/2024 ---\n--- Step: 849/2024 ---\n--- Step: 850/2024 ---\n--- Step: 851/2024 ---\n--- Step: 852/2024 ---\n--- Step: 853/2024 ---\n--- Step: 854/2024 ---\n--- Step: 855/2024 ---\n--- Step: 856/2024 ---\n--- Step: 857/2024 ---\n--- Step: 858/2024 ---\n--- Step: 859/2024 ---\n--- Step: 860/2024 ---\n--- Step: 861/2024 ---\n--- Step: 862/2024 ---\n--- Step: 863/2024 ---\n--- Step: 864/2024 ---\n--- Step: 865/2024 ---\n--- Step: 866/2024 ---\n--- Step: 867/2024 ---\n--- Step: 868/2024 ---\n--- Step: 869/2024 ---\n--- Step: 870/2024 ---\n--- Step: 871/2024 ---\n--- Step: 872/2024 ---\n--- Step: 873/2024 ---\n--- Step: 874/2024 ---\n--- Step: 875/2024 ---\n--- Step: 876/2024 ---\n--- Step: 877/2024 ---\n--- Step: 878/2024 ---\n--- Step: 879/2024 ---\n--- Step: 880/2024 ---\n--- Step: 881/2024 ---\n--- Step: 882/2024 ---\n--- Step: 883/2024 ---\n--- Step: 884/2024 ---\n--- Step: 885/2024 ---\n--- Step: 886/2024 ---\n--- Step: 887/2024 ---\n--- Step: 888/2024 ---\n--- Step: 889/2024 ---\n--- Step: 890/2024 ---\n--- Step: 891/2024 ---\n--- Step: 892/2024 ---\n--- Step: 893/2024 ---\n--- Step: 894/2024 ---\n--- Step: 895/2024 ---\n--- Step: 896/2024 ---\n--- Step: 897/2024 ---\n--- Step: 898/2024 ---\n--- Step: 899/2024 ---\n--- Step: 900/2024 ---\n--- Step: 901/2024 ---\n--- Step: 902/2024 ---\n--- Step: 903/2024 ---\n--- Step: 904/2024 ---\n--- Step: 905/2024 ---\n--- Step: 906/2024 ---\n--- Step: 907/2024 ---\n--- Step: 908/2024 ---\n--- Step: 909/2024 ---\n--- Step: 910/2024 ---\n--- Step: 911/2024 ---\n--- Step: 912/2024 ---\n--- Step: 913/2024 ---\n--- Step: 914/2024 ---\n--- Step: 915/2024 ---\n--- Step: 916/2024 ---\n--- Step: 917/2024 ---\n--- Step: 918/2024 ---\n--- Step: 919/2024 ---\n--- Step: 920/2024 ---\n--- Step: 921/2024 ---\n--- Step: 922/2024 ---\n--- Step: 923/2024 ---\n--- Step: 924/2024 ---\n--- Step: 925/2024 ---\n--- Step: 926/2024 ---\n--- Step: 927/2024 ---\n--- Step: 928/2024 ---\n--- Step: 929/2024 ---\n--- Step: 930/2024 ---\n--- Step: 931/2024 ---\n--- Step: 932/2024 ---\n--- Step: 933/2024 ---\n--- Step: 934/2024 ---\n--- Step: 935/2024 ---\n--- Step: 936/2024 ---\n--- Step: 937/2024 ---\n--- Step: 938/2024 ---\n--- Step: 939/2024 ---\n--- Step: 940/2024 ---\n--- Step: 941/2024 ---\n--- Step: 942/2024 ---\n--- Step: 943/2024 ---\n--- Step: 944/2024 ---\n--- Step: 945/2024 ---\n--- Step: 946/2024 ---\n--- Step: 947/2024 ---\n--- Step: 948/2024 ---\n--- Step: 949/2024 ---\n--- Step: 950/2024 ---\n--- Step: 951/2024 ---\n--- Step: 952/2024 ---\n--- Step: 953/2024 ---\n--- Step: 954/2024 ---\n--- Step: 955/2024 ---\n--- Step: 956/2024 ---\n--- Step: 957/2024 ---\n--- Step: 958/2024 ---\n--- Step: 959/2024 ---\n--- Step: 960/2024 ---\n--- Step: 961/2024 ---\n--- Step: 962/2024 ---\n--- Step: 963/2024 ---\n--- Step: 964/2024 ---\n--- Step: 965/2024 ---\n--- Step: 966/2024 ---\n--- Step: 967/2024 ---\n--- Step: 968/2024 ---\n--- Step: 969/2024 ---\n--- Step: 970/2024 ---\n--- Step: 971/2024 ---\n--- Step: 972/2024 ---\n--- Step: 973/2024 ---\n--- Step: 974/2024 ---\n--- Step: 975/2024 ---\n--- Step: 976/2024 ---\n--- Step: 977/2024 ---\n--- Step: 978/2024 ---\n--- Step: 979/2024 ---\n--- Step: 980/2024 ---\n--- Step: 981/2024 ---\n--- Step: 982/2024 ---\n--- Step: 983/2024 ---\n--- Step: 984/2024 ---\n--- Step: 985/2024 ---\n--- Step: 986/2024 ---\n--- Step: 987/2024 ---\n--- Step: 988/2024 ---\n--- Step: 989/2024 ---\n--- Step: 990/2024 ---\n--- Step: 991/2024 ---\n--- Step: 992/2024 ---\n--- Step: 993/2024 ---\n--- Step: 994/2024 ---\n--- Step: 995/2024 ---\n--- Step: 996/2024 ---\n--- Step: 997/2024 ---\n--- Step: 998/2024 ---\n--- Step: 999/2024 ---\n--- Step: 1000/2024 ---\n--- Step: 1001/2024 ---\n--- Step: 1002/2024 ---\n--- Step: 1003/2024 ---\n--- Step: 1004/2024 ---\n--- Step: 1005/2024 ---\n--- Step: 1006/2024 ---\n--- Step: 1007/2024 ---\n--- Step: 1008/2024 ---\n--- Step: 1009/2024 ---\n--- Step: 1010/2024 ---\n--- Step: 1011/2024 ---\n--- Step: 1012/2024 ---\n--- Step: 1013/2024 ---\n--- Step: 1014/2024 ---\n--- Step: 1015/2024 ---\n--- Step: 1016/2024 ---\n--- Step: 1017/2024 ---\n--- Step: 1018/2024 ---\n--- Step: 1019/2024 ---\n--- Step: 1020/2024 ---\n--- Step: 1021/2024 ---\n--- Step: 1022/2024 ---\n--- Step: 1023/2024 ---\n--- Step: 1024/2024 ---\n--- Step: 1025/2024 ---\n--- Step: 1026/2024 ---\n--- Step: 1027/2024 ---\n--- Step: 1028/2024 ---\n--- Step: 1029/2024 ---\n--- Step: 1030/2024 ---\n--- Step: 1031/2024 ---\n--- Step: 1032/2024 ---\n--- Step: 1033/2024 ---\n--- Step: 1034/2024 ---\n--- Step: 1035/2024 ---\n--- Step: 1036/2024 ---\n--- Step: 1037/2024 ---\n--- Step: 1038/2024 ---\n--- Step: 1039/2024 ---\n--- Step: 1040/2024 ---\n--- Step: 1041/2024 ---\n--- Step: 1042/2024 ---\n--- Step: 1043/2024 ---\n--- Step: 1044/2024 ---\n--- Step: 1045/2024 ---\n--- Step: 1046/2024 ---\n--- Step: 1047/2024 ---\n--- Step: 1048/2024 ---\n--- Step: 1049/2024 ---\n--- Step: 1050/2024 ---\n--- Step: 1051/2024 ---\n--- Step: 1052/2024 ---\n--- Step: 1053/2024 ---\n--- Step: 1054/2024 ---\n--- Step: 1055/2024 ---\n--- Step: 1056/2024 ---\n--- Step: 1057/2024 ---\n--- Step: 1058/2024 ---\n--- Step: 1059/2024 ---\n--- Step: 1060/2024 ---\n--- Step: 1061/2024 ---\n--- Step: 1062/2024 ---\n--- Step: 1063/2024 ---\n--- Step: 1064/2024 ---\n--- Step: 1065/2024 ---\n--- Step: 1066/2024 ---\n--- Step: 1067/2024 ---\n--- Step: 1068/2024 ---\n--- Step: 1069/2024 ---\n--- Step: 1070/2024 ---\n--- Step: 1071/2024 ---\n--- Step: 1072/2024 ---\n--- Step: 1073/2024 ---\n--- Step: 1074/2024 ---\n--- Step: 1075/2024 ---\n--- Step: 1076/2024 ---\n--- Step: 1077/2024 ---\n--- Step: 1078/2024 ---\n--- Step: 1079/2024 ---\n--- Step: 1080/2024 ---\n--- Step: 1081/2024 ---\n--- Step: 1082/2024 ---\n--- Step: 1083/2024 ---\n--- Step: 1084/2024 ---\n--- Step: 1085/2024 ---\n--- Step: 1086/2024 ---\n--- Step: 1087/2024 ---\n--- Step: 1088/2024 ---\n--- Step: 1089/2024 ---\n--- Step: 1090/2024 ---\n--- Step: 1091/2024 ---\n--- Step: 1092/2024 ---\n--- Step: 1093/2024 ---\n--- Step: 1094/2024 ---\n--- Step: 1095/2024 ---\n--- Step: 1096/2024 ---\n--- Step: 1097/2024 ---\n--- Step: 1098/2024 ---\n--- Step: 1099/2024 ---\n--- Step: 1100/2024 ---\n--- Step: 1101/2024 ---\n--- Step: 1102/2024 ---\n--- Step: 1103/2024 ---\n--- Step: 1104/2024 ---\n--- Step: 1105/2024 ---\n--- Step: 1106/2024 ---\n--- Step: 1107/2024 ---\n--- Step: 1108/2024 ---\n--- Step: 1109/2024 ---\n--- Step: 1110/2024 ---\n--- Step: 1111/2024 ---\n--- Step: 1112/2024 ---\n--- Step: 1113/2024 ---\n--- Step: 1114/2024 ---\n--- Step: 1115/2024 ---\n--- Step: 1116/2024 ---\n--- Step: 1117/2024 ---\n--- Step: 1118/2024 ---\n--- Step: 1119/2024 ---\n--- Step: 1120/2024 ---\n--- Step: 1121/2024 ---\n--- Step: 1122/2024 ---\n--- Step: 1123/2024 ---\n--- Step: 1124/2024 ---\n--- Step: 1125/2024 ---\n--- Step: 1126/2024 ---\n--- Step: 1127/2024 ---\n--- Step: 1128/2024 ---\n--- Step: 1129/2024 ---\n--- Step: 1130/2024 ---\n--- Step: 1131/2024 ---\n--- Step: 1132/2024 ---\n--- Step: 1133/2024 ---\n--- Step: 1134/2024 ---\n--- Step: 1135/2024 ---\n--- Step: 1136/2024 ---\n--- Step: 1137/2024 ---\n--- Step: 1138/2024 ---\n--- Step: 1139/2024 ---\n--- Step: 1140/2024 ---\n--- Step: 1141/2024 ---\n--- Step: 1142/2024 ---\n--- Step: 1143/2024 ---\n--- Step: 1144/2024 ---\n--- Step: 1145/2024 ---\n--- Step: 1146/2024 ---\n--- Step: 1147/2024 ---\n--- Step: 1148/2024 ---\n--- Step: 1149/2024 ---\n--- Step: 1150/2024 ---\n--- Step: 1151/2024 ---\n--- Step: 1152/2024 ---\n--- Step: 1153/2024 ---\n--- Step: 1154/2024 ---\n--- Step: 1155/2024 ---\n--- Step: 1156/2024 ---\n--- Step: 1157/2024 ---\n--- Step: 1158/2024 ---\n--- Step: 1159/2024 ---\n--- Step: 1160/2024 ---\n--- Step: 1161/2024 ---\n--- Step: 1162/2024 ---\n--- Step: 1163/2024 ---\n--- Step: 1164/2024 ---\n--- Step: 1165/2024 ---\n--- Step: 1166/2024 ---\n--- Step: 1167/2024 ---\n--- Step: 1168/2024 ---\n--- Step: 1169/2024 ---\n--- Step: 1170/2024 ---\n--- Step: 1171/2024 ---\n--- Step: 1172/2024 ---\n--- Step: 1173/2024 ---\n--- Step: 1174/2024 ---\n--- Step: 1175/2024 ---\n--- Step: 1176/2024 ---\n--- Step: 1177/2024 ---\n--- Step: 1178/2024 ---\n--- Step: 1179/2024 ---\n--- Step: 1180/2024 ---\n--- Step: 1181/2024 ---\n--- Step: 1182/2024 ---\n--- Step: 1183/2024 ---\n--- Step: 1184/2024 ---\n--- Step: 1185/2024 ---\n--- Step: 1186/2024 ---\n--- Step: 1187/2024 ---\n--- Step: 1188/2024 ---\n--- Step: 1189/2024 ---\n--- Step: 1190/2024 ---\n--- Step: 1191/2024 ---\n--- Step: 1192/2024 ---\n--- Step: 1193/2024 ---\n--- Step: 1194/2024 ---\n--- Step: 1195/2024 ---\n--- Step: 1196/2024 ---\n--- Step: 1197/2024 ---\n--- Step: 1198/2024 ---\n--- Step: 1199/2024 ---\n--- Step: 1200/2024 ---\n--- Step: 1201/2024 ---\n--- Step: 1202/2024 ---\n--- Step: 1203/2024 ---\n--- Step: 1204/2024 ---\n--- Step: 1205/2024 ---\n--- Step: 1206/2024 ---\n--- Step: 1207/2024 ---\n--- Step: 1208/2024 ---\n--- Step: 1209/2024 ---\n--- Step: 1210/2024 ---\n--- Step: 1211/2024 ---\n--- Step: 1212/2024 ---\n--- Step: 1213/2024 ---\n--- Step: 1214/2024 ---\n--- Step: 1215/2024 ---\n--- Step: 1216/2024 ---\n--- Step: 1217/2024 ---\n--- Step: 1218/2024 ---\n--- Step: 1219/2024 ---\n--- Step: 1220/2024 ---\n--- Step: 1221/2024 ---\n--- Step: 1222/2024 ---\n--- Step: 1223/2024 ---\n--- Step: 1224/2024 ---\n--- Step: 1225/2024 ---\n--- Step: 1226/2024 ---\n--- Step: 1227/2024 ---\n--- Step: 1228/2024 ---\n--- Step: 1229/2024 ---\n--- Step: 1230/2024 ---\n--- Step: 1231/2024 ---\n--- Step: 1232/2024 ---\n--- Step: 1233/2024 ---\n--- Step: 1234/2024 ---\n--- Step: 1235/2024 ---\n--- Step: 1236/2024 ---\n--- Step: 1237/2024 ---\n--- Step: 1238/2024 ---\n--- Step: 1239/2024 ---\n--- Step: 1240/2024 ---\n--- Step: 1241/2024 ---\n--- Step: 1242/2024 ---\n--- Step: 1243/2024 ---\n--- Step: 1244/2024 ---\n--- Step: 1245/2024 ---\n--- Step: 1246/2024 ---\n--- Step: 1247/2024 ---\n--- Step: 1248/2024 ---\n--- Step: 1249/2024 ---\n--- Step: 1250/2024 ---\n--- Step: 1251/2024 ---\n--- Step: 1252/2024 ---\n--- Step: 1253/2024 ---\n--- Step: 1254/2024 ---\n--- Step: 1255/2024 ---\n--- Step: 1256/2024 ---\n--- Step: 1257/2024 ---\n--- Step: 1258/2024 ---\n--- Step: 1259/2024 ---\n--- Step: 1260/2024 ---\n--- Step: 1261/2024 ---\n--- Step: 1262/2024 ---\n--- Step: 1263/2024 ---\n--- Step: 1264/2024 ---\n--- Step: 1265/2024 ---\n--- Step: 1266/2024 ---\n--- Step: 1267/2024 ---\n--- Step: 1268/2024 ---\n--- Step: 1269/2024 ---\n--- Step: 1270/2024 ---\n--- Step: 1271/2024 ---\n--- Step: 1272/2024 ---\n--- Step: 1273/2024 ---\n--- Step: 1274/2024 ---\n--- Step: 1275/2024 ---\n--- Step: 1276/2024 ---\n--- Step: 1277/2024 ---\n--- Step: 1278/2024 ---\n--- Step: 1279/2024 ---\n--- Step: 1280/2024 ---\n--- Step: 1281/2024 ---\n--- Step: 1282/2024 ---\n--- Step: 1283/2024 ---\n--- Step: 1284/2024 ---\n--- Step: 1285/2024 ---\n--- Step: 1286/2024 ---\n--- Step: 1287/2024 ---\n--- Step: 1288/2024 ---\n--- Step: 1289/2024 ---\n--- Step: 1290/2024 ---\n--- Step: 1291/2024 ---\n--- Step: 1292/2024 ---\n--- Step: 1293/2024 ---\n--- Step: 1294/2024 ---\n--- Step: 1295/2024 ---\n--- Step: 1296/2024 ---\n--- Step: 1297/2024 ---\n--- Step: 1298/2024 ---\n--- Step: 1299/2024 ---\n--- Step: 1300/2024 ---\n--- Step: 1301/2024 ---\n--- Step: 1302/2024 ---\n--- Step: 1303/2024 ---\n--- Step: 1304/2024 ---\n--- Step: 1305/2024 ---\n--- Step: 1306/2024 ---\n--- Step: 1307/2024 ---\n--- Step: 1308/2024 ---\n--- Step: 1309/2024 ---\n--- Step: 1310/2024 ---\n--- Step: 1311/2024 ---\n--- Step: 1312/2024 ---\n--- Step: 1313/2024 ---\n--- Step: 1314/2024 ---\n--- Step: 1315/2024 ---\n--- Step: 1316/2024 ---\n--- Step: 1317/2024 ---\n--- Step: 1318/2024 ---\n--- Step: 1319/2024 ---\n--- Step: 1320/2024 ---\n--- Step: 1321/2024 ---\n--- Step: 1322/2024 ---\n--- Step: 1323/2024 ---\n--- Step: 1324/2024 ---\n--- Step: 1325/2024 ---\n--- Step: 1326/2024 ---\n--- Step: 1327/2024 ---\n--- Step: 1328/2024 ---\n--- Step: 1329/2024 ---\n--- Step: 1330/2024 ---\n--- Step: 1331/2024 ---\n--- Step: 1332/2024 ---\n--- Step: 1333/2024 ---\n--- Step: 1334/2024 ---\n--- Step: 1335/2024 ---\n--- Step: 1336/2024 ---\n--- Step: 1337/2024 ---\n--- Step: 1338/2024 ---\n--- Step: 1339/2024 ---\n--- Step: 1340/2024 ---\n--- Step: 1341/2024 ---\n--- Step: 1342/2024 ---\n--- Step: 1343/2024 ---\n--- Step: 1344/2024 ---\n--- Step: 1345/2024 ---\n--- Step: 1346/2024 ---\n--- Step: 1347/2024 ---\n--- Step: 1348/2024 ---\n--- Step: 1349/2024 ---\n--- Step: 1350/2024 ---\n--- Step: 1351/2024 ---\n--- Step: 1352/2024 ---\n--- Step: 1353/2024 ---\n--- Step: 1354/2024 ---\n--- Step: 1355/2024 ---\n--- Step: 1356/2024 ---\n--- Step: 1357/2024 ---\n--- Step: 1358/2024 ---\n--- Step: 1359/2024 ---\n--- Step: 1360/2024 ---\n--- Step: 1361/2024 ---\n--- Step: 1362/2024 ---\n--- Step: 1363/2024 ---\n--- Step: 1364/2024 ---\n--- Step: 1365/2024 ---\n--- Step: 1366/2024 ---\n--- Step: 1367/2024 ---\n--- Step: 1368/2024 ---\n--- Step: 1369/2024 ---\n--- Step: 1370/2024 ---\n--- Step: 1371/2024 ---\n--- Step: 1372/2024 ---\n--- Step: 1373/2024 ---\n--- Step: 1374/2024 ---\n--- Step: 1375/2024 ---\n--- Step: 1376/2024 ---\n--- Step: 1377/2024 ---\n--- Step: 1378/2024 ---\n--- Step: 1379/2024 ---\n--- Step: 1380/2024 ---\n--- Step: 1381/2024 ---\n--- Step: 1382/2024 ---\n--- Step: 1383/2024 ---\n--- Step: 1384/2024 ---\n--- Step: 1385/2024 ---\n--- Step: 1386/2024 ---\n--- Step: 1387/2024 ---\n--- Step: 1388/2024 ---\n--- Step: 1389/2024 ---\n--- Step: 1390/2024 ---\n--- Step: 1391/2024 ---\n--- Step: 1392/2024 ---\n--- Step: 1393/2024 ---\n--- Step: 1394/2024 ---\n--- Step: 1395/2024 ---\n--- Step: 1396/2024 ---\n--- Step: 1397/2024 ---\n--- Step: 1398/2024 ---\n--- Step: 1399/2024 ---\n--- Step: 1400/2024 ---\n--- Step: 1401/2024 ---\n--- Step: 1402/2024 ---\n--- Step: 1403/2024 ---\n--- Step: 1404/2024 ---\n--- Step: 1405/2024 ---\n--- Step: 1406/2024 ---\n--- Step: 1407/2024 ---\n--- Step: 1408/2024 ---\n--- Step: 1409/2024 ---\n--- Step: 1410/2024 ---\n--- Step: 1411/2024 ---\n--- Step: 1412/2024 ---\n--- Step: 1413/2024 ---\n--- Step: 1414/2024 ---\n--- Step: 1415/2024 ---\n--- Step: 1416/2024 ---\n--- Step: 1417/2024 ---\n--- Step: 1418/2024 ---\n--- Step: 1419/2024 ---\n--- Step: 1420/2024 ---\n--- Step: 1421/2024 ---\n--- Step: 1422/2024 ---\n--- Step: 1423/2024 ---\n--- Step: 1424/2024 ---\n--- Step: 1425/2024 ---\n--- Step: 1426/2024 ---\n--- Step: 1427/2024 ---\n--- Step: 1428/2024 ---\n--- Step: 1429/2024 ---\n--- Step: 1430/2024 ---\n--- Step: 1431/2024 ---\n--- Step: 1432/2024 ---\n--- Step: 1433/2024 ---\n--- Step: 1434/2024 ---\n--- Step: 1435/2024 ---\n--- Step: 1436/2024 ---\n--- Step: 1437/2024 ---\n--- Step: 1438/2024 ---\n--- Step: 1439/2024 ---\n--- Step: 1440/2024 ---\n--- Step: 1441/2024 ---\n--- Step: 1442/2024 ---\n--- Step: 1443/2024 ---\n--- Step: 1444/2024 ---\n--- Step: 1445/2024 ---\n--- Step: 1446/2024 ---\n--- Step: 1447/2024 ---\n--- Step: 1448/2024 ---\n--- Step: 1449/2024 ---\n--- Step: 1450/2024 ---\n--- Step: 1451/2024 ---\n--- Step: 1452/2024 ---\n--- Step: 1453/2024 ---\n--- Step: 1454/2024 ---\n--- Step: 1455/2024 ---\n--- Step: 1456/2024 ---\n--- Step: 1457/2024 ---\n--- Step: 1458/2024 ---\n--- Step: 1459/2024 ---\n--- Step: 1460/2024 ---\n--- Step: 1461/2024 ---\n--- Step: 1462/2024 ---\n--- Step: 1463/2024 ---\n--- Step: 1464/2024 ---\n--- Step: 1465/2024 ---\n--- Step: 1466/2024 ---\n--- Step: 1467/2024 ---\n--- Step: 1468/2024 ---\n--- Step: 1469/2024 ---\n--- Step: 1470/2024 ---\n--- Step: 1471/2024 ---\n--- Step: 1472/2024 ---\n--- Step: 1473/2024 ---\n--- Step: 1474/2024 ---\n--- Step: 1475/2024 ---\n--- Step: 1476/2024 ---\n--- Step: 1477/2024 ---\n--- Step: 1478/2024 ---\n--- Step: 1479/2024 ---\n--- Step: 1480/2024 ---\n--- Step: 1481/2024 ---\n--- Step: 1482/2024 ---\n--- Step: 1483/2024 ---\n--- Step: 1484/2024 ---\n--- Step: 1485/2024 ---\n--- Step: 1486/2024 ---\n--- Step: 1487/2024 ---\n--- Step: 1488/2024 ---\n--- Step: 1489/2024 ---\n--- Step: 1490/2024 ---\n--- Step: 1491/2024 ---\n--- Step: 1492/2024 ---\n--- Step: 1493/2024 ---\n--- Step: 1494/2024 ---\n--- Step: 1495/2024 ---\n--- Step: 1496/2024 ---\n--- Step: 1497/2024 ---\n--- Step: 1498/2024 ---\n--- Step: 1499/2024 ---\n--- Step: 1500/2024 ---\n--- Step: 1501/2024 ---\n--- Step: 1502/2024 ---\n--- Step: 1503/2024 ---\n--- Step: 1504/2024 ---\n--- Step: 1505/2024 ---\n--- Step: 1506/2024 ---\n--- Step: 1507/2024 ---\n--- Step: 1508/2024 ---\n--- Step: 1509/2024 ---\n--- Step: 1510/2024 ---\n--- Step: 1511/2024 ---\n--- Step: 1512/2024 ---\n--- Step: 1513/2024 ---\n--- Step: 1514/2024 ---\n--- Step: 1515/2024 ---\n--- Step: 1516/2024 ---\n--- Step: 1517/2024 ---\n--- Step: 1518/2024 ---\n--- Step: 1519/2024 ---\n--- Step: 1520/2024 ---\n--- Step: 1521/2024 ---\n--- Step: 1522/2024 ---\n--- Step: 1523/2024 ---\n--- Step: 1524/2024 ---\n--- Step: 1525/2024 ---\n--- Step: 1526/2024 ---\n--- Step: 1527/2024 ---\n--- Step: 1528/2024 ---\n--- Step: 1529/2024 ---\n--- Step: 1530/2024 ---\n--- Step: 1531/2024 ---\n--- Step: 1532/2024 ---\n--- Step: 1533/2024 ---\n--- Step: 1534/2024 ---\n--- Step: 1535/2024 ---\n--- Step: 1536/2024 ---\n--- Step: 1537/2024 ---\n--- Step: 1538/2024 ---\n--- Step: 1539/2024 ---\n--- Step: 1540/2024 ---\n--- Step: 1541/2024 ---\n--- Step: 1542/2024 ---\n--- Step: 1543/2024 ---\n--- Step: 1544/2024 ---\n--- Step: 1545/2024 ---\n--- Step: 1546/2024 ---\n--- Step: 1547/2024 ---\n--- Step: 1548/2024 ---\n--- Step: 1549/2024 ---\n--- Step: 1550/2024 ---\n--- Step: 1551/2024 ---\n--- Step: 1552/2024 ---\n--- Step: 1553/2024 ---\n--- Step: 1554/2024 ---\n--- Step: 1555/2024 ---\n--- Step: 1556/2024 ---\n--- Step: 1557/2024 ---\n--- Step: 1558/2024 ---\n--- Step: 1559/2024 ---\n--- Step: 1560/2024 ---\n--- Step: 1561/2024 ---\n--- Step: 1562/2024 ---\n--- Step: 1563/2024 ---\n--- Step: 1564/2024 ---\n--- Step: 1565/2024 ---\n--- Step: 1566/2024 ---\n--- Step: 1567/2024 ---\n--- Step: 1568/2024 ---\n--- Step: 1569/2024 ---\n--- Step: 1570/2024 ---\n--- Step: 1571/2024 ---\n--- Step: 1572/2024 ---\n--- Step: 1573/2024 ---\n--- Step: 1574/2024 ---\n--- Step: 1575/2024 ---\n--- Step: 1576/2024 ---\n--- Step: 1577/2024 ---\n--- Step: 1578/2024 ---\n--- Step: 1579/2024 ---\n--- Step: 1580/2024 ---\n--- Step: 1581/2024 ---\n--- Step: 1582/2024 ---\n--- Step: 1583/2024 ---\n--- Step: 1584/2024 ---\n--- Step: 1585/2024 ---\n--- Step: 1586/2024 ---\n--- Step: 1587/2024 ---\n--- Step: 1588/2024 ---\n--- Step: 1589/2024 ---\n--- Step: 1590/2024 ---\n--- Step: 1591/2024 ---\n--- Step: 1592/2024 ---\n--- Step: 1593/2024 ---\n--- Step: 1594/2024 ---\n--- Step: 1595/2024 ---\n--- Step: 1596/2024 ---\n--- Step: 1597/2024 ---\n--- Step: 1598/2024 ---\n--- Step: 1599/2024 ---\n--- Step: 1600/2024 ---\n--- Step: 1601/2024 ---\n--- Step: 1602/2024 ---\n--- Step: 1603/2024 ---\n--- Step: 1604/2024 ---\n--- Step: 1605/2024 ---\n--- Step: 1606/2024 ---\n--- Step: 1607/2024 ---\n--- Step: 1608/2024 ---\n--- Step: 1609/2024 ---\n--- Step: 1610/2024 ---\n--- Step: 1611/2024 ---\n--- Step: 1612/2024 ---\n--- Step: 1613/2024 ---\n--- Step: 1614/2024 ---\n--- Step: 1615/2024 ---\n--- Step: 1616/2024 ---\n--- Step: 1617/2024 ---\n--- Step: 1618/2024 ---\n--- Step: 1619/2024 ---\n--- Step: 1620/2024 ---\n--- Step: 1621/2024 ---\n--- Step: 1622/2024 ---\n--- Step: 1623/2024 ---\n--- Step: 1624/2024 ---\n--- Step: 1625/2024 ---\n--- Step: 1626/2024 ---\n--- Step: 1627/2024 ---\n--- Step: 1628/2024 ---\n--- Step: 1629/2024 ---\n--- Step: 1630/2024 ---\n--- Step: 1631/2024 ---\n--- Step: 1632/2024 ---\n--- Step: 1633/2024 ---\n--- Step: 1634/2024 ---\n--- Step: 1635/2024 ---\n--- Step: 1636/2024 ---\n--- Step: 1637/2024 ---\n--- Step: 1638/2024 ---\n--- Step: 1639/2024 ---\n--- Step: 1640/2024 ---\n--- Step: 1641/2024 ---\n--- Step: 1642/2024 ---\n--- Step: 1643/2024 ---\n--- Step: 1644/2024 ---\n--- Step: 1645/2024 ---\n--- Step: 1646/2024 ---\n--- Step: 1647/2024 ---\n--- Step: 1648/2024 ---\n--- Step: 1649/2024 ---\n--- Step: 1650/2024 ---\n--- Step: 1651/2024 ---\n--- Step: 1652/2024 ---\n--- Step: 1653/2024 ---\n--- Step: 1654/2024 ---\n--- Step: 1655/2024 ---\n--- Step: 1656/2024 ---\n--- Step: 1657/2024 ---\n--- Step: 1658/2024 ---\n--- Step: 1659/2024 ---\n--- Step: 1660/2024 ---\n--- Step: 1661/2024 ---\n--- Step: 1662/2024 ---\n--- Step: 1663/2024 ---\n--- Step: 1664/2024 ---\n--- Step: 1665/2024 ---\n--- Step: 1666/2024 ---\n--- Step: 1667/2024 ---\n--- Step: 1668/2024 ---\n--- Step: 1669/2024 ---\n--- Step: 1670/2024 ---\n--- Step: 1671/2024 ---\n--- Step: 1672/2024 ---\n--- Step: 1673/2024 ---\n--- Step: 1674/2024 ---\n--- Step: 1675/2024 ---\n--- Step: 1676/2024 ---\n--- Step: 1677/2024 ---\n--- Step: 1678/2024 ---\n--- Step: 1679/2024 ---\n--- Step: 1680/2024 ---\n--- Step: 1681/2024 ---\n--- Step: 1682/2024 ---\n--- Step: 1683/2024 ---\n--- Step: 1684/2024 ---\n--- Step: 1685/2024 ---\n--- Step: 1686/2024 ---\n--- Step: 1687/2024 ---\n--- Step: 1688/2024 ---\n--- Step: 1689/2024 ---\n--- Step: 1690/2024 ---\n--- Step: 1691/2024 ---\n--- Step: 1692/2024 ---\n--- Step: 1693/2024 ---\n--- Step: 1694/2024 ---\n--- Step: 1695/2024 ---\n--- Step: 1696/2024 ---\n--- Step: 1697/2024 ---\n--- Step: 1698/2024 ---\n--- Step: 1699/2024 ---\n--- Step: 1700/2024 ---\n--- Step: 1701/2024 ---\n--- Step: 1702/2024 ---\n--- Step: 1703/2024 ---\n--- Step: 1704/2024 ---\n--- Step: 1705/2024 ---\n--- Step: 1706/2024 ---\n--- Step: 1707/2024 ---\n--- Step: 1708/2024 ---\n--- Step: 1709/2024 ---\n--- Step: 1710/2024 ---\n--- Step: 1711/2024 ---\n--- Step: 1712/2024 ---\n--- Step: 1713/2024 ---\n--- Step: 1714/2024 ---\n--- Step: 1715/2024 ---\n--- Step: 1716/2024 ---\n--- Step: 1717/2024 ---\n--- Step: 1718/2024 ---\n--- Step: 1719/2024 ---\n--- Step: 1720/2024 ---\n--- Step: 1721/2024 ---\n--- Step: 1722/2024 ---\n--- Step: 1723/2024 ---\n--- Step: 1724/2024 ---\n--- Step: 1725/2024 ---\n--- Step: 1726/2024 ---\n--- Step: 1727/2024 ---\n--- Step: 1728/2024 ---\n--- Step: 1729/2024 ---\n--- Step: 1730/2024 ---\n--- Step: 1731/2024 ---\n--- Step: 1732/2024 ---\n--- Step: 1733/2024 ---\n--- Step: 1734/2024 ---\n--- Step: 1735/2024 ---\n--- Step: 1736/2024 ---\n--- Step: 1737/2024 ---\n--- Step: 1738/2024 ---\n--- Step: 1739/2024 ---\n--- Step: 1740/2024 ---\n--- Step: 1741/2024 ---\n--- Step: 1742/2024 ---\n--- Step: 1743/2024 ---\n--- Step: 1744/2024 ---\n--- Step: 1745/2024 ---\n--- Step: 1746/2024 ---\n--- Step: 1747/2024 ---\n--- Step: 1748/2024 ---\n--- Step: 1749/2024 ---\n--- Step: 1750/2024 ---\n--- Step: 1751/2024 ---\n--- Step: 1752/2024 ---\n--- Step: 1753/2024 ---\n--- Step: 1754/2024 ---\n--- Step: 1755/2024 ---\n--- Step: 1756/2024 ---\n--- Step: 1757/2024 ---\n--- Step: 1758/2024 ---\n--- Step: 1759/2024 ---\n--- Step: 1760/2024 ---\n--- Step: 1761/2024 ---\n--- Step: 1762/2024 ---\n--- Step: 1763/2024 ---\n--- Step: 1764/2024 ---\n--- Step: 1765/2024 ---\n--- Step: 1766/2024 ---\n--- Step: 1767/2024 ---\n--- Step: 1768/2024 ---\n--- Step: 1769/2024 ---\n--- Step: 1770/2024 ---\n--- Step: 1771/2024 ---\n--- Step: 1772/2024 ---\n--- Step: 1773/2024 ---\n--- Step: 1774/2024 ---\n--- Step: 1775/2024 ---\n--- Step: 1776/2024 ---\n--- Step: 1777/2024 ---\n--- Step: 1778/2024 ---\n--- Step: 1779/2024 ---\n--- Step: 1780/2024 ---\n--- Step: 1781/2024 ---\n--- Step: 1782/2024 ---\n--- Step: 1783/2024 ---\n--- Step: 1784/2024 ---\n--- Step: 1785/2024 ---\n--- Step: 1786/2024 ---\n--- Step: 1787/2024 ---\n--- Step: 1788/2024 ---\n--- Step: 1789/2024 ---\n--- Step: 1790/2024 ---\n--- Step: 1791/2024 ---\n--- Step: 1792/2024 ---\n--- Step: 1793/2024 ---\n--- Step: 1794/2024 ---\n--- Step: 1795/2024 ---\n--- Step: 1796/2024 ---\n--- Step: 1797/2024 ---\n--- Step: 1798/2024 ---\n--- Step: 1799/2024 ---\n--- Step: 1800/2024 ---\n--- Step: 1801/2024 ---\n--- Step: 1802/2024 ---\n--- Step: 1803/2024 ---\n--- Step: 1804/2024 ---\n--- Step: 1805/2024 ---\n--- Step: 1806/2024 ---\n--- Step: 1807/2024 ---\n--- Step: 1808/2024 ---\n--- Step: 1809/2024 ---\n--- Step: 1810/2024 ---\n--- Step: 1811/2024 ---\n--- Step: 1812/2024 ---\n--- Step: 1813/2024 ---\n--- Step: 1814/2024 ---\n--- Step: 1815/2024 ---\n--- Step: 1816/2024 ---\n--- Step: 1817/2024 ---\n--- Step: 1818/2024 ---\n--- Step: 1819/2024 ---\n--- Step: 1820/2024 ---\n--- Step: 1821/2024 ---\n--- Step: 1822/2024 ---\n--- Step: 1823/2024 ---\n--- Step: 1824/2024 ---\n--- Step: 1825/2024 ---\n--- Step: 1826/2024 ---\n--- Step: 1827/2024 ---\n--- Step: 1828/2024 ---\n--- Step: 1829/2024 ---\n--- Step: 1830/2024 ---\n--- Step: 1831/2024 ---\n--- Step: 1832/2024 ---\n--- Step: 1833/2024 ---\n--- Step: 1834/2024 ---\n--- Step: 1835/2024 ---\n--- Step: 1836/2024 ---\n--- Step: 1837/2024 ---\n--- Step: 1838/2024 ---\n--- Step: 1839/2024 ---\n--- Step: 1840/2024 ---\n--- Step: 1841/2024 ---\n--- Step: 1842/2024 ---\n--- Step: 1843/2024 ---\n--- Step: 1844/2024 ---\n--- Step: 1845/2024 ---\n--- Step: 1846/2024 ---\n--- Step: 1847/2024 ---\n--- Step: 1848/2024 ---\n--- Step: 1849/2024 ---\n--- Step: 1850/2024 ---\n--- Step: 1851/2024 ---\n--- Step: 1852/2024 ---\n--- Step: 1853/2024 ---\n--- Step: 1854/2024 ---\n--- Step: 1855/2024 ---\n--- Step: 1856/2024 ---\n--- Step: 1857/2024 ---\n--- Step: 1858/2024 ---\n--- Step: 1859/2024 ---\n--- Step: 1860/2024 ---\n--- Step: 1861/2024 ---\n--- Step: 1862/2024 ---\n--- Step: 1863/2024 ---\n--- Step: 1864/2024 ---\n--- Step: 1865/2024 ---\n--- Step: 1866/2024 ---\n--- Step: 1867/2024 ---\n--- Step: 1868/2024 ---\n--- Step: 1869/2024 ---\n--- Step: 1870/2024 ---\n--- Step: 1871/2024 ---\n--- Step: 1872/2024 ---\n--- Step: 1873/2024 ---\n--- Step: 1874/2024 ---\n--- Step: 1875/2024 ---\n--- Step: 1876/2024 ---\n--- Step: 1877/2024 ---\n--- Step: 1878/2024 ---\n--- Step: 1879/2024 ---\n--- Step: 1880/2024 ---\n--- Step: 1881/2024 ---\n--- Step: 1882/2024 ---\n--- Step: 1883/2024 ---\n--- Step: 1884/2024 ---\n--- Step: 1885/2024 ---\n--- Step: 1886/2024 ---\n--- Step: 1887/2024 ---\n--- Step: 1888/2024 ---\n--- Step: 1889/2024 ---\n--- Step: 1890/2024 ---\n--- Step: 1891/2024 ---\n--- Step: 1892/2024 ---\n--- Step: 1893/2024 ---\n--- Step: 1894/2024 ---\n--- Step: 1895/2024 ---\n--- Step: 1896/2024 ---\n--- Step: 1897/2024 ---\n--- Step: 1898/2024 ---\n--- Step: 1899/2024 ---\n--- Step: 1900/2024 ---\n--- Step: 1901/2024 ---\n--- Step: 1902/2024 ---\n--- Step: 1903/2024 ---\n--- Step: 1904/2024 ---\n--- Step: 1905/2024 ---\n--- Step: 1906/2024 ---\n--- Step: 1907/2024 ---\n--- Step: 1908/2024 ---\n--- Step: 1909/2024 ---\n--- Step: 1910/2024 ---\n--- Step: 1911/2024 ---\n--- Step: 1912/2024 ---\n--- Step: 1913/2024 ---\n--- Step: 1914/2024 ---\n--- Step: 1915/2024 ---\n--- Step: 1916/2024 ---\n--- Step: 1917/2024 ---\n--- Step: 1918/2024 ---\n--- Step: 1919/2024 ---\n--- Step: 1920/2024 ---\n--- Step: 1921/2024 ---\n--- Step: 1922/2024 ---\n--- Step: 1923/2024 ---\n--- Step: 1924/2024 ---\n--- Step: 1925/2024 ---\n--- Step: 1926/2024 ---\n--- Step: 1927/2024 ---\n--- Step: 1928/2024 ---\n--- Step: 1929/2024 ---\n--- Step: 1930/2024 ---\n--- Step: 1931/2024 ---\n--- Step: 1932/2024 ---\n--- Step: 1933/2024 ---\n--- Step: 1934/2024 ---\n--- Step: 1935/2024 ---\n--- Step: 1936/2024 ---\n--- Step: 1937/2024 ---\n--- Step: 1938/2024 ---\n--- Step: 1939/2024 ---\n--- Step: 1940/2024 ---\n--- Step: 1941/2024 ---\n--- Step: 1942/2024 ---\n--- Step: 1943/2024 ---\n--- Step: 1944/2024 ---\n--- Step: 1945/2024 ---\n--- Step: 1946/2024 ---\n--- Step: 1947/2024 ---\n--- Step: 1948/2024 ---\n--- Step: 1949/2024 ---\n--- Step: 1950/2024 ---\n--- Step: 1951/2024 ---\n--- Step: 1952/2024 ---\n--- Step: 1953/2024 ---\n--- Step: 1954/2024 ---\n--- Step: 1955/2024 ---\n--- Step: 1956/2024 ---\n--- Step: 1957/2024 ---\n--- Step: 1958/2024 ---\n--- Step: 1959/2024 ---\n--- Step: 1960/2024 ---\n--- Step: 1961/2024 ---\n--- Step: 1962/2024 ---\n--- Step: 1963/2024 ---\n--- Step: 1964/2024 ---\n--- Step: 1965/2024 ---\n--- Step: 1966/2024 ---\n--- Step: 1967/2024 ---\n--- Step: 1968/2024 ---\n--- Step: 1969/2024 ---\n--- Step: 1970/2024 ---\n--- Step: 1971/2024 ---\n--- Step: 1972/2024 ---\n--- Step: 1973/2024 ---\n--- Step: 1974/2024 ---\n--- Step: 1975/2024 ---\n--- Step: 1976/2024 ---\n--- Step: 1977/2024 ---\n--- Step: 1978/2024 ---\n--- Step: 1979/2024 ---\n--- Step: 1980/2024 ---\n--- Step: 1981/2024 ---\n--- Step: 1982/2024 ---\n--- Step: 1983/2024 ---\n--- Step: 1984/2024 ---\n--- Step: 1985/2024 ---\n--- Step: 1986/2024 ---\n--- Step: 1987/2024 ---\n--- Step: 1988/2024 ---\n--- Step: 1989/2024 ---\n--- Step: 1990/2024 ---\n--- Step: 1991/2024 ---\n--- Step: 1992/2024 ---\n--- Step: 1993/2024 ---\n--- Step: 1994/2024 ---\n--- Step: 1995/2024 ---\n--- Step: 1996/2024 ---\n--- Step: 1997/2024 ---\n--- Step: 1998/2024 ---\n--- Step: 1999/2024 ---\n--- Step: 2000/2024 ---\n--- Step: 2001/2024 ---\n--- Step: 2002/2024 ---\n--- Step: 2003/2024 ---\n--- Step: 2004/2024 ---\n--- Step: 2005/2024 ---\n--- Step: 2006/2024 ---\n--- Step: 2007/2024 ---\n--- Step: 2008/2024 ---\n--- Step: 2009/2024 ---\n--- Step: 2010/2024 ---\n--- Step: 2011/2024 ---\n--- Step: 2012/2024 ---\n--- Step: 2013/2024 ---\n--- Step: 2014/2024 ---\n--- Step: 2015/2024 ---\n--- Step: 2016/2024 ---\n--- Step: 2017/2024 ---\n--- Step: 2018/2024 ---\n--- Step: 2019/2024 ---\n--- Step: 2020/2024 ---\n--- Step: 2021/2024 ---\n--- Step: 2022/2024 ---\n--- Step: 2023/2024 ---\n","output_type":"stream"},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1747' max='1747' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1747/1747 01:39]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test Results: {'eval_loss': 0.8607094883918762, 'eval_runtime': 99.2738, 'eval_samples_per_second': 140.732, 'eval_steps_per_second': 17.598, 'epoch': 0.9997530254383798}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"wandb_key = '7fce0d03dfc1e78160dcfe859983acc039b0a5be'","metadata":{"id":"SmqNtxHDbl-S","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T19:28:53.481427Z","iopub.execute_input":"2024-12-22T19:28:53.481753Z","iopub.status.idle":"2024-12-22T19:28:53.485355Z","shell.execute_reply.started":"2024-12-22T19:28:53.481729Z","shell.execute_reply":"2024-12-22T19:28:53.484525Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MultiHeadDifferentialAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.d_model = config.n_embd\n        self.n_head = config.n_head\n        assert self.d_model % self.n_head == 0\n        \n        self.d_head = self.d_model // self.n_head\n        \n        # Parameters for Q1, Q2, K1, K2, V\n        self.W_q = nn.Linear(self.d_model, 2 * self.d_model)\n        self.W_k = nn.Linear(self.d_model, 2 * self.d_model)\n        self.W_v = nn.Linear(self.d_model, 2 * self.d_model)\n        \n        # Learnable parameters for ÃÂ»\n        self.lambda_q1 = nn.Parameter(torch.randn(self.d_head))\n        self.lambda_k1 = nn.Parameter(torch.randn(self.d_head))\n        self.lambda_q2 = nn.Parameter(torch.randn(self.d_head))\n        self.lambda_k2 = nn.Parameter(torch.randn(self.d_head))\n        self.lambda_init = 0.8\n        \n        self.dropout = nn.Dropout(config.attn_pdrop)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n        self.proj = nn.Linear(self.d_model, self.d_model)\n        \n        # For compatibility with GPT-2\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_ctx, config.n_ctx)).view(\n            1, 1, config.n_ctx, config.n_ctx))\n\n    def compute_lambda(self, batch_size):\n        lambda_term1 = torch.exp(self.lambda_q1 @ self.lambda_k1)\n        lambda_term2 = torch.exp(self.lambda_q2 @ self.lambda_k2)\n        lambda_val = lambda_term1 - lambda_term2 + self.lambda_init\n        return lambda_val.view(1, 1, 1).expand(batch_size, -1, -1)\n\n    def split_heads(self, x):\n        new_shape = x.size()[:-1] + (self.n_head, self.d_head)\n        x = x.view(*new_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        layer_past=None,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        use_cache=False,\n        output_attentions=False,\n    ):\n        batch_size, seq_length, _ = hidden_states.size()\n\n        # Compute Q1, Q2, K1, K2, V\n        qkv_outputs = []\n        for proj in [self.W_q, self.W_k, self.W_v]:\n            output = proj(hidden_states)\n            if proj != self.W_v:\n                output = output.chunk(2, dim=-1)\n                qkv_outputs.extend(output)\n            else:\n                qkv_outputs.append(output)\n        \n        Q1, Q2, K1, K2, V = qkv_outputs\n        \n        # Split heads\n        Q1, Q2 = map(self.split_heads, [Q1, Q2])\n        K1, K2 = map(self.split_heads, [K1, K2])\n        V = self.split_heads(V)\n        \n        # Handle past key values\n        if layer_past is not None:\n            past_k1, past_k2, past_v = layer_past\n            K1 = torch.cat([past_k1, K1], dim=-2)\n            K2 = torch.cat([past_k2, K2], dim=-2)\n            V = torch.cat([past_v, V], dim=-2)\n        \n        if use_cache:\n            present = (K1, K2, V)\n        else:\n            present = None\n\n        # Scale attention scores\n        scaling = math.sqrt(self.d_head)\n        \n        # Compute attention scores for both pairs\n        attn_1 = torch.matmul(Q1, K1.transpose(-2, -1)) / scaling\n        attn_2 = torch.matmul(Q2, K2.transpose(-2, -1)) / scaling\n        \n        # Apply causal mask\n        causal_mask = self.bias[:, :, :seq_length, :K1.size(-2)]\n        attn_1 = attn_1.masked_fill(causal_mask == 0, float('-inf'))\n        attn_2 = attn_2.masked_fill(causal_mask == 0, float('-inf'))\n        \n        # Apply attention mask if provided\n        if attention_mask is not None:\n            attn_1 = attn_1 + attention_mask\n            attn_2 = attn_2 + attention_mask\n        \n        # Compute softmax\n        attn_1 = F.softmax(attn_1, dim=-1)\n        attn_2 = F.softmax(attn_2, dim=-1)\n        \n        # Apply dropout\n        attn_1 = self.dropout(attn_1)\n        attn_2 = self.dropout(attn_2)\n        \n        # Compute lambda\n        lambda_val = self.compute_lambda(batch_size)\n        \n        # Compute differential attention\n        y = torch.matmul(attn_1, V) - lambda_val * torch.matmul(attn_2, V)\n        \n        # Restore sequence dimension and combine heads\n        y = y.permute(0, 2, 1, 3).contiguous()\n        y = y.view(batch_size, seq_length, self.d_model)\n        \n        # Final projection and dropout\n        y = self.resid_dropout(self.proj(y))\n        \n        outputs = (y, present)\n        if output_attentions:\n            outputs += ((attn_1, attn_2),)\n        \n        return outputs  # output, present, (attentions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:04:33.070756Z","iopub.execute_input":"2024-12-22T22:04:33.071102Z","iopub.status.idle":"2024-12-22T22:04:33.085508Z","shell.execute_reply.started":"2024-12-22T22:04:33.071051Z","shell.execute_reply":"2024-12-22T22:04:33.084685Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nsaved_model = AutoModelForCausalLM.from_pretrained(\"/kaggle/working/arabic-qa-gpt2-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:10:13.018280Z","iopub.execute_input":"2024-12-22T22:10:13.018581Z","iopub.status.idle":"2024-12-22T22:10:13.079466Z","shell.execute_reply.started":"2024-12-22T22:10:13.018558Z","shell.execute_reply":"2024-12-22T22:10:13.078851Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, pipeline\n\nqa_pipeline = pipeline(\n    task=\"text-generation\",\n    model=saved_model,\n    tokenizer=tokenizer,\n    device=0 if torch.cuda.is_available() else -1\n)\n\ncontext = \"Ø§Ø®ØªØ±Ø¹ Ø£Ù„ÙƒØ³Ù†Ø¯Ø± ØºØ±Ø§Ù‡Ø§Ù… Ø¨ÙŠÙ„ Ø§Ù„Ù‡Ø§ØªÙ ÙÙŠ Ø¹Ø§Ù… 1876.\"\nquestion = \"Ù…Ù† Ù‡Ùˆ Ù…Ø®ØªØ±Ø¹ Ø§Ù„Ù‡Ø§ØªÙØŸ\"\ninput_text = f\"{question} {context}\"\n\nresponse = qa_pipeline(\n    input_text,\n    max_length=120,\n    num_return_sequences=1,\n    pad_token_id=tokenizer.pad_token_id\n)\nprint(response[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:11:12.037554Z","iopub.execute_input":"2024-12-22T22:11:12.037885Z","iopub.status.idle":"2024-12-22T22:11:27.234876Z","shell.execute_reply.started":"2024-12-22T22:11:12.037856Z","shell.execute_reply":"2024-12-22T22:11:27.233974Z"}},"outputs":[{"name":"stdout","text":"Ù…Ù† Ù‡Ùˆ Ù…Ø®ØªØ±Ø¹ Ø§Ù„Ù‡Ø§ØªÙØŸ Ø§Ø®ØªØ±Ø¹ Ø£Ù„ÙƒØ³Ù†Ø¯Ø± ØºØ±Ø§Ù‡Ø§Ù… Ø¨ÙŠÙ„ Ø§Ù„Ù‡Ø§ØªÙ ÙÙŠ Ø¹Ø§Ù… 1876.ï¿½Ùï¿½ï¿½ï¿½  Â«ï¿½ï¿½)Ø§ï¿½ Ù…Ù† Ø¹Ù„Ù‰ Ø¢Ø®Ø± (Âï¿½ Ø­ØªÙ‰ Ø¥Ù„Ù‰\" -ÙˆØ±.Ø¹ Ù‡Ø°Ø§ ÙƒØ§Ù†ØªØ©!Ù‡Ø± ÙƒØ§Ù† Ù‚Ø¯ Ø£ÙŠ Ø§Ù„Ø§Ø³Ù… Ø¬Ø§Ø¦Ø²Ø©ÙŠÙ† 2 Ù‡ÙŠ ÙˆØ§Ø­Ø¯ : Ø± Ø§Ø³Ù… \" Ùˆ ÙˆÙ‡Ùˆ Ø¨ÙŠÙ† ÙƒÙˆØ¯ S ÙˆÙ‡ÙŠØ³ ÙƒÙ„Ù…Ø© ÙˆØ± Ù„Ùˆ ÙÙŠÙ‡ ÙˆÙ…Ø§ Ù…Ù‡Ù… Ø£Ù† Ø£Ùˆ Ø°Ù„Ùƒ Ø¨Ø¹Ø¶ ÙˆØ¬ÙˆØ¯ ÙˆÙ„Ù…Ø§Ø°Ø§ ÙˆÙƒØ£Ù†Ù‡Ø§Ù‡Ø§ ÙˆÙ‚Ø¯ Ù…Ø«Ù„ ØºÙŠØ± Ø§Ù„Ø¨Ø¹Ø¶ Ù„Ù‡ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªÙŠ ÙŠØ­Ø¯Ø« Ø¹Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø­ Ù‡Ø°Ù‡ Ù„Ø£Ù† ÙƒÙ…Ø§'ÙˆÙ‚ÙØ© Ø§Ù„ÙˆØµÙ â€œ ÙˆÙ…Ù† Ù„Ù„Ø¹ ÙÙŠÙ† ÙŠØ·Ø±Ù‚ï¿½ Ø¯ÙˆØ± ÙˆØ­ØªÙ‰ ÙˆØ¹ ØŒ ÙˆØµÙ„ Ù…Ø´Ø§Ù‡Ø¯ Ø§Ù„Ø¨Ø§Ø¨ Ø¹ Ø§Ù† Ø¨Ø§Ø³Ù… Ø§ÙŠ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù„Ù‚Ø¨ut Ø§Ø¹ØªØ¨Ø± Ø§Ù„Ø°ÙŠØµÙ Ù…Ø¹ Ø´Ù‡Ø§Ø¯Ø© 3 ØªÙ‚ÙŠÙŠÙ… Ù…Ø­Ù…Ø¯ ÙˆØ¶Ø¹ Ù„Ø´ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø±Ø¯ Ø¹Ø±Ø¶Ø±ÙŠÙ… ÙƒÙˆØ± ÙŠØµÙ„ 4 ÙˆØµÙ„Øª ØªÙƒØ±ÙŠÙ… ÙˆØ¹ÙˆØ¯Ø© Ø§Ø³Øª Ù„Ø± ÙˆÙˆØµÙ„Øª ÙˆØµØ§Ø±Øª Ø§Ø¨Ù†Ù‡Ø§ Ù†Ø§Ù„ Ù†Ù‚Ø§Ø¨Ø© Ø­Ù‚ÙˆÙ‚ ÙˆØ­Ù‚ÙˆÙ‚. ÙˆØ§Ù„Ø­Ù‚ÙˆÙ‚ØµØ§Ù Ø­Ù…Ø§ÙŠØ© Ø­Ù‚ ÙˆØ­Ù‚ ÙˆØ­Ù…Ø§ÙŠØ© ÙˆÙ…Ù†Ø­ Ù…Ù†Ø­ ÙˆØ¬Ø§Ø¦Ø²Ø© ÙˆØ³Ø§Ù… Ø§Ù„ØªÙƒØ±ÙŠÙ… Ø­ØµÙ„ Ù…ØµÙŠØ± Ø§Ù„Ù…ØµÙŠØ± Ø·ÙŠØ§Ø± Ø§Ù„Ø·ÙŠØ§Ø± Ø§Ù„Ø·ÙŠØ±Ø§Ù† Ø§Ù„Ø¬ÙˆÙŠ Ø·Ø§Ø¦Ø±Ø§Øª Ø§Ù„Ø¬ÙˆÙŠØ© Ø§Ù„Ø·Ø§Ø¦Ø±Ø§Øª Ù„Ù„Ø·Ø§Ø¦Ø±Ø§Øª Ù„Ù„Ø·ÙŠØ±Ø§Ù† Ø·Ø§Ø¦Ø±Ø© Ø·Ø§Ø¦Ø±Ø§Ø± Ø§Ù„Ø·Ø§Ø¦Ø±Ø© Ø·ÙŠØ±Ø§Ù† Ø¬ÙˆÙŠØ© Ø§Ù„Ø¬Ùˆ Ø§Ù„Ø¬ÙˆÙŠ Ø§Ù„Ù…Ø·Ø± Ø§Ø­ØªØ¬ Ø§Ø­ØªØ¬ Ø§Ø­ØªØ¬ Ø§Ø¶ Ø§Ù†Ù Ø§Ø¯ Ø§Ø¯ ØºÙƒØ±ÙŠ Â« ÙƒØ§Ø¯ÙƒÙˆØ±ÙŠÙˆÙ†ÙŠØ¯Ø§... ÙƒÙˆÙ… ÙˆØ±Ø¯ Ù†ÙŠØ± ØªÙ‡Ø¯ÙŠØ¯ ØªÙ‡Ø¯ÙŠØ¯ ØªÙ‡Ø¯ÙŠØ¯ Ø§Ù†ØªÙ‡Ø§Ùƒ ØªÙ‡Ø¯ÙŠØ¯ ØªÙ‡Ø¯ÙŠØ¯ ØªØ­Ø°ÙŠØ± ØªÙ‡Ø¯ÙŠØ¯ ØªÙ‡Ø¯ÙŠØ¯ Ø¹Ø¯ Â« ØªÙ‡Ø¯ÙŠØ¯ ØªÙ‡Ø¯ÙŠØ¯ Ø¥Ù†Ø°Ø§Ø± Â« ØªÙ‡Ø¯ÙŠØ¯ Ø§Ù‚ØªØ±Ø§Ø­ Â« ØªÙ‡Ø¯ÙŠØ¯ Ø§Ù„Ø§Ù†Ø°Ø§Ø± Â« ØªØ­Ø°ÙŠØ± ØªØ­Ø°ÙŠØ± Ø§Ù„Ø­Ø¯ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø­Ø¯ ØªØ­Ø°ÙŠØ± Ø§Ù„ØªØ­Ø°ÙŠØ± ØªØ­Ø°ÙŠØ± ØªØ­Ø°ÙŠØ± Ù…Ù†Ø¹ ØªØ­Ø°ÙŠØ± ØªØ­Ø°ÙŠØ± ØªØ­Ø°ÙŠØ± Ø­Ø±Ù… Ø­Ø±Ù… Ù…Ù‚Ø§Ù… Ø­Ø±Ù… Ø­Ø±Ù… Ø§Ù„Ø­Ø±Ù… Ø°ÙƒØ± Ù†Ø¨Ù‡ruØ±Ù…Ù‡ÙØ±Ø©Ù†Ø­Ø·ï¿½ Ø§Ù„ï¿½ Ø§Ù„Ù…Ø§Ø¡ Ø§Ù„Ù…ÙŠØ§Ù‡ ÙˆØ§Ù„Ù…ÙŠØ§Ù‡ Ù„Ù„Ù…ÙŠØ§Ù‡ Ù„Ù„Ù…Ø§Ø¡ Ø§Ù„Ù…Ø§Ø¦ÙŠ Ù…Ø§Ø¦ÙŠ Ø§Ù„Ù…Ø§Ø¦ÙŠØ© Ø¨Ø§Ù„Ù…Ø§Ø¡ Ø´ÙØ§ÙØ© Ø­Ø±Ø§Ø±Ø© Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø¯Ø±Ø¬Ø§Øª ÙˆØ¯Ø±Ø¬Ø© Ø·Ø¨Ù‚Ø© Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø´Ù…Ø³ Ø¶ÙˆØ¡ ØªÙ‚Ø±ÙŠØ¨ Ø¶ÙˆØ¦ÙŠØ© Ø§Ù„Ø·Ø§Ù‚Ø© Ø·Ø§Ù‚Ø© Ø§Ù„Ø¬Ø³ÙŠÙ…Ø§Øª Ø°Ø±Ø§Øª Ø°Ø±Ø© Ø§Ù„ÙƒØ±Ø¨ÙˆÙ†Ø±ÙˆØ¬ÙŠÙ† Ø§Ù„Ø°Ø®ÙˆØ¬ÙŠÙ†Ù„ÙƒØªØ±ÙˆÙ†Ø§ØªÙ‚Ø©Ø±ÙŠ 1 Ø§Ù„Ø¥Ù„ÙƒØªØ± Ø§Ù„Ø¬Ø²ÙŠØ¦Ø§Øª Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¦ÙŠØ© Ø¬Ø²ÙŠØ¦Ø§Øª Ù…Ø­Ø¯Ø¯Ø©ÙŠÙ„%1 Ø§Ù„Ù‡ÙŠØ¯Ø±ÙˆØ¬ÙŠÙ†Ø§Ù„ ÙƒÙ„ Ù…Ø§Ù†ï¿½ Ø§Ù„Ø£Ø±Ø¶Øª/ Ø°Ø§ØªÙŠØ©Ø¯ Ø§Ù„Ù…,-Ø§Øª Ø§Ù„Ø³ Ù„Ø§ Ø¨Ø´ÙƒÙ„ Ø§Ù„ÙÙˆÙˆÙ† Ø§Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ Ø£ÙƒØ«Ø±ØµØ¨Ø« Ø®Ù„Ø§Ù„ ÙƒØ¨ÙŠØ± Ø¹Ù„ÙŠÙ‡Ø§ ÙˆÙÙŠÙ‡Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø£Ø®Ø±Ù‰ Ø¹Ø¯Ø¯ Ù…ØªØ±Ø¹Ø© Ø¨ï¿½ ØªØ­Øª Ø§Ù„Ø¹Ø§Ù…Ø© Ù‚Ø¯Ù… Ù…Ù„ÙŠÙˆÙ† Ø® Ù…Ù„ÙŠØ§Ø± Ù„Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ù„Ù„Øª Ù„Ø§Ù†Øª Øª 11 Ù…ØµØ±ÙŠ 20 Ø§Ù„Ø¹Ø§Ù…Ù†ÙŠÙ† 21 25 ÙˆÙ„Ø§ Ù‚7 Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ù‡Ø§ Ù„ÙƒØ±Ø© Ø§Ù„Ø®Ø·97 Ø§Ù„Ø³ÙƒØ§Ù† ÙÙ…Ø«Ù„Ø© Ø§Ù„Ø¹Ø¯ÙŠØ¯Ùª Ø­ÙŠØ« Ø¹Ø´Ø±Ø¬ Ø§Ù„ÙƒØ«ÙŠØ±Ù„ÙŠØ§Ø¦Ù„Ù„ÙŠÙ†ÙˆÙ„Ù„ÙŠØ©Ø²Ø­Ù„ Ù‚Ø¨Ù„ÙŠØ± Ø£ÙˆÙ„ÙˆØ±ÙŠ Ø§Ù„Ù…ØªØ­Ø¯Ø©\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"def replace_attention_layers(model, ratio=0.3):\n    \"\"\"\n    Replace a portion of the model's attention layers with differential attention\n\n    Args:\n        model: GPT-2 model\n        ratio: Portion of layers to replace (between 0.25 and 0.5)\n    \"\"\"\n    assert 0.25 <= ratio <= 0.5, \"Ratio must be between 0.25 and 0.5\"\n\n    n_layers = len(model.transformer.h)\n\n    n_replace = int(n_layers * ratio)\n\n    layers_to_replace = torch.randperm(n_layers)[:n_replace].sort()[0].tolist()\n\n    for layer_idx in layers_to_replace:\n        layer = model.transformer.h[layer_idx]\n        # Replace the attention module\n        layer.attn = MultiHeadDifferentialAttention(model.config)\n\n    return model, layers_to_replace","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:06:00.512174Z","iopub.execute_input":"2024-12-22T22:06:00.512503Z","iopub.status.idle":"2024-12-22T22:06:00.517219Z","shell.execute_reply.started":"2024-12-22T22:06:00.512480Z","shell.execute_reply":"2024-12-22T22:06:00.516322Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"#new_model = replace_attention_layers(saved_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T21:17:56.893245Z","iopub.execute_input":"2024-12-22T21:17:56.893537Z","iopub.status.idle":"2024-12-22T21:17:56.979731Z","shell.execute_reply.started":"2024-12-22T21:17:56.893516Z","shell.execute_reply":"2024-12-22T21:17:56.979091Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Usage example\nif __name__ == \"__main__\":\n    # Paths to your data files\n    \"\n    \n    # Train the modified model\n    \n    \n    # Optional: Compare original and modified model outputs\n    original_model, tokenizer = load_finetuned_model()\n    modified_model = AutoModelForCausalLM.from_pretrained(\"./arabic-qa-gpt2-differential-final\")\n    \n    # Example comparison\n    question = \"Ù…Ù† Ù‡Ùˆ Ù…Ø®ØªØ±Ø¹ Ø§Ù„Ù‡Ø§ØªÙØŸ\"\n    context = \"Ø§Ø®ØªØ±Ø¹ Ø£Ù„ÙƒØ³Ù†Ø¯Ø± ØºØ±Ø§Ù‡Ø§Ù… Ø¨ÙŠÙ„ Ø§Ù„Ù‡Ø§ØªÙ ÙÙŠ Ø¹Ø§Ù… 1876.\"\n    \n    comparison = compare_models(\n        question, context, \n        original_model, modified_model, \n        tokenizer\n    )\n    print(\"\\nComparison of model outputs:\")\n    print(comparison)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n\nfrom torch.utils.data import Dataset\n\nimport json\n\n \n\nmodel1 = AutoModelForCausalLM.from_pretrained(\"./arabic-qa-gpt2-finetuned\")\n\ntokenizer1 = AutoTokenizer.from_pretrained(\"./arabic-qa-gpt2-finetuned\")\n\n \n\n# Tokenize train and validation data\n\ntrain_data = tokenize_data(train_contexts, train_questions, train_answers, tokenizer1)\n\nval_data = tokenize_data(val_contexts, val_questions, val_answers, tokenizer1)\n\ntest_data = tokenize_data(test_contexts, test_questions, test_answers, tokenizer1)\n\n\n\n\n# Modify model with differential attention\n\ndef prepare_modified_model(original_model, replacement_ratio=0.3):\n\n    # Import our previously defined differential attention class\n\n    model, replaced_layers = replace_attention_layers(original_model, ratio=replacement_ratio)\n\n    print(f\"Replaced attention in layers: {replaced_layers}\")\n\n    return model\n\n \n\n# Training arguments for the modified model\n\ndef get_training_args():\n\n    return TrainingArguments(\n\n        output_dir=\"./arabic-qa-gpt2-differential\",\n\n        num_train_epochs=1,  # Fewer epochs since model is already fine-tuned\n\n        per_device_train_batch_size=8,\n\n        per_device_eval_batch_size=8,\n\n        warmup_steps=200,\n\n        weight_decay=0.01,\n\n        logging_dir=\"./logs_differential\",\n\n        logging_steps=50,\n\n        save_steps=500,\n\n        eval_steps=500,\n\n        evaluation_strategy=\"steps\",\n\n        load_best_model_at_end=True,\n\n        metric_for_best_model=\"loss\",\n\n        greater_is_better=False,\n\n        # Add gradient clipping to help stabilize training\n\n        max_grad_norm=1.0,\n\n        # Add learning rate scheduling\n\n        learning_rate=1e-5,\n        \n        report_to=\"none\"\n    )\n\n \n\n# Main training function for modified model\n\ndef train_modified_model(train_data, val_data, test_data,model):\n\n \n\n   \n\n    # 2. Replace attention layers\n\n    modified_model = prepare_modified_model(model)\n\n   \n\n    # 3. Load datasets (using our previous ArabicQADataset class)\n\n    train_dataset = QADataset(train_data)\n\n    val_dataset = QADataset(val_data)\n\n    test_dataset = QADataset(test_data)\n\n   \n\n    # 4. Initialize trainer\n\n    training_args = get_training_args()\n\n    trainer = Trainer(\n\n        model=modified_model,\n\n        args=training_args,\n\n        train_dataset=train_dataset,\n\n        eval_dataset=val_dataset,\n\n    )\n\n   \n\n    # 5. Train the model\n\n    print(\"Starting training of modified model...\")\n\n    trainer.train()\n\n   \n\n    # 6. Evaluate on test set\n\n    print(\"Evaluating on test set...\")\n\n    test_results = trainer.evaluate(test_dataset)\n\n    print(f\"Test Results: {test_results}\")\n\n   \n\n    # 7. Save the modified model\n\n    modified_model.save_pretrained(\"./arabic-qa-gpt2-differential-final\")\n\n    tokenizer1.save_pretrained(\"./arabic-qa-gpt2-differential-final\")\n\n   \n\n    return test_results\n\n \n\n# Function to compare model outputs\n\ndef compare_models(question, context, original_model, modified_model, tokenizer):\n\n    input_text = f\"{question} [SEP] {context} [SEP]\"\n\n    inputs = tokenizer(input_text, return_tensors=\"pt\")\n\n   \n\n    # Generate from original model\n\n    original_output = original_model.generate(\n\n        **inputs,\n\n        max_length=150,\n\n        num_return_sequences=1,\n\n        no_repeat_ngram_size=2\n\n    )\n\n   \n\n    # Generate from modified model\n\n    modified_output = modified_model.generate(\n\n        **inputs,\n\n        max_length=150,\n\n        num_return_sequences=1,\n\n        no_repeat_ngram_size=2\n\n    )\n\n   \n\n    return {\n\n        \"Original\": tokenizer.decode(original_output[0]),\n\n        \"Modified\": tokenizer.decode(modified_output[0])\n\n    }\n\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:06:03.582246Z","iopub.execute_input":"2024-12-22T22:06:03.582530Z","iopub.status.idle":"2024-12-22T22:08:02.096832Z","shell.execute_reply.started":"2024-12-22T22:06:03.582509Z","shell.execute_reply":"2024-12-22T22:08:02.095892Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"results = train_modified_model(train_data, val_data, test_data,model1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T22:08:19.817165Z","iopub.execute_input":"2024-12-22T22:08:19.817454Z","iopub.status.idle":"2024-12-22T22:08:20.537044Z","shell.execute_reply.started":"2024-12-22T22:08:19.817434Z","shell.execute_reply":"2024-12-22T22:08:20.535726Z"}},"outputs":[{"name":"stdout","text":"Replaced attention in layers: [0, 4, 8]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Starting training of modified model...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-3efd5bb0524d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_modified_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-52-23edfaba0560>\u001b[0m in \u001b[0;36mtrain_modified_model\u001b[0;34m(train_data, val_data, test_data, model)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training of modified model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2279\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3320\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1316\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1127\u001b[0m                 )\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m   1130\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    615\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-e1d6e87d429d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mQ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mQ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mK1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Handle past key values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-e1d6e87d429d>\u001b[0m in \u001b[0;36msplit_heads\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mnew_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[8, 128, 12, 64]' is invalid for input of size 1572864"],"ename":"RuntimeError","evalue":"shape '[8, 128, 12, 64]' is invalid for input of size 1572864","output_type":"error"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}